# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

import torch
import torch.nn as nn
from tensordict import TensorDict
from torch.distributions import Normal
from typing import Any, NoReturn

from rsl_rl.networks import MLP, EmpiricalNormalization, create_r2plus1d_feature_extractor, create_transformer_fusion_actor



class ActorCriticElevationNet(nn.Module):
    is_recurrent: bool = False

    def __init__(
        self,
        obs: TensorDict,
        obs_groups: dict[str, list[str]],
        num_actions: int,
        actor_obs_normalization: bool = False,
        critic_obs_normalization: bool = False,
        actor_hidden_dims: tuple[int] | list[int] = [256, 256, 256],
        critic_hidden_dims: tuple[int] | list[int] = [256, 256, 256],
        activation: str = "elu",
        init_noise_std: float = 1.0,
        noise_std_type: str = "scalar",
        state_dependent_std: bool = False,
        # R(2+1)D è§†è§‰ç¼–ç å™¨é…ç½®
        vision_input_channels: int = 1,
        vision_feature_dim: int = 64,
        vision_num_frames: int = 5,
        vision_spatial_size: tuple[int, int] = (11, 11),
        # æœ¬ä½“ä¿¡æ¯ç¼–ç å™¨é…ç½®
        proprio_feature_dim: int = 128,
        # Transformer Fusion é…ç½®
        transformer_hidden_dim: int = 256,
        transformer_num_heads: int = 4,
        transformer_num_layers: int = 2,
        transformer_mlp_hidden_dims: list[int] | None = None,
        transformer_dropout: float = 0.1,
        transformer_use_proprio_embedding: bool = True,
        transformer_use_vision_embedding: bool = True,
        **kwargs: dict[str, Any],
    ) -> None:
        if kwargs:
            print(
                "ActorCritic.__init__ got unexpected arguments, which will be ignored: " + str([key for key in kwargs])
            )
        super().__init__()

        # ä¼ é€’å›žEnvçš„é¢å¤–ä¿¡æ¯
        self.extra_info = dict()

        # Get the observation dimensions
        self.obs_groups = obs_groups
        self.vision_spatial_size = vision_spatial_size  # ä¿å­˜ç”¨äºŽreshape
        num_actor_obs = 0
        for obs_group in obs_groups["policy"]:
            # æ·±åº¦å›¾æ˜¯4Då¼ é‡ [batch, frames, height, width]ï¼Œè·³è¿‡ç»´åº¦ç´¯åŠ 
            if obs_group == "height_scan_history":
                continue
            assert len(obs[obs_group].shape) == 2, f"The observation '{obs_group}' must be 1D (got shape {obs[obs_group].shape})."
            num_actor_obs += obs[obs_group].shape[-1]
        
        num_critic_obs = 0
        for obs_group in obs_groups["critic"]:
            # æ·±åº¦å›¾æ˜¯4Då¼ é‡ï¼Œè·³è¿‡ç»´åº¦ç´¯åŠ 
            if obs_group == "height_scan_history":
                continue
            assert len(obs[obs_group].shape) == 2, f"The observation '{obs_group}' must be 1D (got shape {obs[obs_group].shape})."
            num_critic_obs += obs[obs_group].shape[-1]

        self.state_dependent_std = state_dependent_std

        ########################################## Actor ##############################################
        # éªŒè¯height_scan_historyå­˜åœ¨
        has_depth_input = False
        for group_name, group_keys in obs_groups.items():
            if "height_scan_history" in group_keys:
                has_depth_input = True
                break
        # å¦‚æžœæ²¡æœ‰åœ¨obs_groupsä¸­æ‰¾åˆ°ï¼Œæ£€æŸ¥æ˜¯å¦ç›´æŽ¥åœ¨obsä¸­
        if not has_depth_input:
            has_depth_input = "height_scan_history" in obs.sorted_keys
        
        if not has_depth_input:
            raise ValueError(
                "ActorCriticElevationNet requires 'height_scan_history' in observations. "
                "Please add 'height_scan_history' to one of the obs_groups "
                "(e.g., obs_groups['policy'] or obs_groups['perception']). "
                "If you don't need vision input, please use RslRlPpoActorCriticCfg instead."
            )
        
        # éªŒè¯é…ç½®
        if True:  # å§‹ç»ˆä½¿ç”¨Transformer Fusionæž¶æž„
            # 1. éªŒè¯height_scan_historyçš„å½¢çŠ¶
            if "height_scan_history" not in obs:
                raise ValueError(
                    "height_scan_history is configured in obs_groups but not found in obs. "
                    "Please check your environment observation configuration."
                )
            
            # æå–å®žé™…çš„æ·±åº¦å¼ é‡
            # height_scan_historyå¯èƒ½æ˜¯åµŒå¥—çš„TensorDictï¼Œéœ€è¦é€’å½’æå–
            depth_obs = obs["height_scan_history"]
            while isinstance(depth_obs, TensorDict):
                # ä¸€ç›´è§£åŒ…ç›´åˆ°èŽ·å¾—çœŸæ­£çš„Tensor
                keys = list(depth_obs.keys())
                if not keys:
                    raise ValueError("height_scan_history is an empty TensorDict")
                depth_obs = depth_obs[keys[0]]
            
            # éªŒè¯å½¢çŠ¶ï¼šå›ºå®šä¸º [batch, frames, height*width]
            if len(depth_obs.shape) != 3:
                raise ValueError(
                    f"height_scan_history must be 3D tensor [batch, frames, height*width], "
                    f"but got shape {depth_obs.shape}"
                )
            
            batch_size, actual_frames, flattened_size = depth_obs.shape
            expected_height, expected_width = vision_spatial_size
            expected_size = expected_height * expected_width
            
            # éªŒè¯å±•å¼€å¤§å°
            if flattened_size != expected_size:
                raise ValueError(
                    f"height_scan_history has flattened spatial size {flattened_size}, "
                    f"but expected {expected_size} (from vision_spatial_size {vision_spatial_size}). "
                    f"Please check your configuration."
                )
            
            # 2. éªŒè¯å¸§æ•°
            if actual_frames != vision_num_frames:
                raise ValueError(
                    f"height_scan_history has {actual_frames} frames, "
                    f"but vision_num_frames is configured as {vision_num_frames}. "
                    f"Please adjust vision_num_frames in your config to match."
                )
            
            # 4. éªŒè¯Transformerå‚æ•°
            if transformer_hidden_dim % transformer_num_heads != 0:
                raise ValueError(
                    f"transformer_hidden_dim ({transformer_hidden_dim}) must be divisible by "
                    f"transformer_num_heads ({transformer_num_heads})"
                )
            
            # 5. ä¸æ”¯æŒstate_dependent_std
            if state_dependent_std:
                raise NotImplementedError(
                    "state_dependent_std is not supported with Transformer Fusion architecture. "
                    "Please set state_dependent_std=False in your config."
                )
            
            print("\n" + "="*80)
            print("ðŸŒŸ Using Transformer Fusion Architecture with Depth Input")
            print("="*80)
            print(f"âœ“ Validated height_scan_history: [batch, {actual_frames}, {expected_height}Ã—{expected_width}={expected_size}] (will reshape to [batch, {actual_frames}, {expected_height}, {expected_width}])")
            
            # 1. æœ¬ä½“ä¿¡æ¯å¤„ç†æµ - MLPç‰¹å¾æå–å™¨
            self.proprio_encoder = MLP(num_actor_obs, proprio_feature_dim, actor_hidden_dims, activation)
            print(f"âœ“ Proprioception Encoder MLP: {num_actor_obs} -> {proprio_feature_dim}")
            
            # 2. æ·±åº¦å›¾åºåˆ—å¤„ç†æµ - R(2+1)Dç‰¹å¾æå–å™¨
            self.elevation_net = create_r2plus1d_feature_extractor(
                input_channels=vision_input_channels,
                output_dim=vision_feature_dim,
                num_frames=vision_num_frames,
                spatial_size=vision_spatial_size
            )
            print(f"âœ“ Vision Encoder R(2+1)D: [{vision_input_channels}, {vision_num_frames}, {vision_spatial_size[0]}, {vision_spatial_size[1]}] -> {vision_feature_dim}")
            
            # 3. Transformer Fusionæ¨¡å— + MLPæ˜ å°„åˆ°åŠ¨ä½œ
            if transformer_mlp_hidden_dims is None:
                transformer_mlp_hidden_dims = [256, 128]
            
            self.fusion_actor = create_transformer_fusion_actor(
                proprioception_dim=proprio_feature_dim,
                vision_feature_dim=vision_feature_dim,
                num_actions=num_actions,
                hidden_dim=transformer_hidden_dim,
                num_heads=transformer_num_heads,
                num_layers=transformer_num_layers,
                mlp_hidden_dims=transformer_mlp_hidden_dims,
                dropout=transformer_dropout,
                use_proprio_embedding=transformer_use_proprio_embedding,
                use_vision_embedding=transformer_use_vision_embedding,
            )
            print("="*80 + "\n")

        # Actor observation normalization
        self.actor_obs_normalization = actor_obs_normalization
        if actor_obs_normalization:
            self.actor_obs_normalizer = EmpiricalNormalization(num_actor_obs)
        else:
            self.actor_obs_normalizer = torch.nn.Identity()


        ########################################## Critic ##############################################
        self.critic = MLP(num_critic_obs, 1, critic_hidden_dims, activation)
        print(f"Critic MLP: {self.critic}")

        # Critic observation normalization
        self.critic_obs_normalization = critic_obs_normalization
        if critic_obs_normalization:
            self.critic_obs_normalizer = EmpiricalNormalization(num_critic_obs)
        else:
            self.critic_obs_normalizer = torch.nn.Identity()

        # Action noise
        self.noise_std_type = noise_std_type
        # Transformer Fusionæž¶æž„ä¸æ”¯æŒstate_dependent_stdï¼ˆå·²åœ¨å‰é¢éªŒè¯ï¼‰
        if self.noise_std_type == "scalar":
            self.std = nn.Parameter(init_noise_std * torch.ones(num_actions))
        elif self.noise_std_type == "log":
            self.log_std = nn.Parameter(torch.log(init_noise_std * torch.ones(num_actions)))
        else:
            raise ValueError(f"Unknown standard deviation type: {self.noise_std_type}. Should be 'scalar' or 'log'")

        # Action distribution
        # Note: Populated in update_distribution
        self.distribution = None

        # Disable args validation for speedup
        Normal.set_default_validate_args(False)

    def reset(self, dones: torch.Tensor | None = None) -> None:
        pass

    def forward(self) -> NoReturn:
        raise NotImplementedError

    @property
    def action_mean(self) -> torch.Tensor:
        return self.distribution.mean

    @property
    def action_std(self) -> torch.Tensor:
        return self.distribution.stddev

    @property
    def entropy(self) -> torch.Tensor:
        return self.distribution.entropy().sum(dim=-1)

    def _update_distribution(self, obs: TensorDict, proprio_obs: torch.Tensor) -> None:
        """æ›´æ–°åŠ¨ä½œåˆ†å¸ƒ
        
        Args:
            obs: åŽŸå§‹è§‚æµ‹TensorDictï¼ˆåŒ…å«æ·±åº¦å›¾ç­‰ï¼‰
            proprio_obs: å·²å¤„ç†çš„æœ¬ä½“è§‚æµ‹ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
        """
        # 1. æå–æœ¬ä½“ç‰¹å¾
        proprio_features = self.proprio_encoder(proprio_obs)  # [batch, 128]
        
        # 2. æå–æ·±åº¦å›¾å¹¶å¤„ç†
        depth_obs = obs["height_scan_history"]
        while isinstance(depth_obs, TensorDict):
            # ä¸€ç›´è§£åŒ…ç›´åˆ°èŽ·å¾—çœŸæ­£çš„Tensor
            keys = list(depth_obs.keys())
            if not keys:
                raise ValueError("height_scan_history is an empty TensorDict")
            depth_obs = depth_obs[keys[0]]
        
        # éªŒè¯è§£åŒ…åŽçš„å½¢çŠ¶
        if not isinstance(depth_obs, torch.Tensor):
            raise TypeError(f"After unpacking, depth_obs should be a Tensor, but got {type(depth_obs)}")
        
        if len(depth_obs.shape) != 3:
            raise ValueError(
                f"depth_obs should have 3 dimensions [batch, frames, height*width], "
                f"but got shape {depth_obs.shape}"
            )
        
        # Reshape: [batch, frames, height*width] -> [batch, frames, height, width]
        batch_size, num_frames, _ = depth_obs.shape
        expected_height, expected_width = self.vision_spatial_size
        depth_obs = depth_obs.view(batch_size, num_frames, expected_height, expected_width)
        
        vision_features = self.elevation_net(depth_obs)  # [batch, 64]
        
        # 3. Transformerèžåˆå¹¶ç”ŸæˆåŠ¨ä½œå‡å€¼
        mean = self.fusion_actor(proprio_features, vision_features)  # [batch, num_actions]
        
        # 4. è®¡ç®—æ ‡å‡†å·®
        if self.noise_std_type == "scalar":
            std = self.std.expand_as(mean)
        elif self.noise_std_type == "log":
            std = torch.exp(self.log_std).expand_as(mean)
        else:
            raise ValueError(f"Unknown standard deviation type: {self.noise_std_type}")
        
        # Create distribution
        self.distribution = Normal(mean, std)

    def act(self, obs: TensorDict, **kwargs: dict[str, Any]) -> torch.Tensor:
        proprio_obs = self.get_actor_obs(obs)
        proprio_obs = self.actor_obs_normalizer(proprio_obs)
        self._update_distribution(obs, proprio_obs)
        return self.distribution.sample(), self.extra_info

    def act_inference(self, obs: TensorDict) -> torch.Tensor:
        proprio_obs = self.get_actor_obs(obs)
        proprio_obs = self.actor_obs_normalizer(proprio_obs)
        
        # Transformer Fusionæž¶æž„
        proprio_features = self.proprio_encoder(proprio_obs)
        
        # æå–æ·±åº¦å›¾å¹¶å¤„ç†
        depth_obs = obs["height_scan_history"]
        while isinstance(depth_obs, TensorDict):
            # ä¸€ç›´è§£åŒ…ç›´åˆ°èŽ·å¾—çœŸæ­£çš„Tensor
            keys = list(depth_obs.keys())
            if not keys:
                raise ValueError("height_scan_history is an empty TensorDict")
            depth_obs = depth_obs[keys[0]]
        
        # éªŒè¯è§£åŒ…åŽçš„å½¢çŠ¶
        if not isinstance(depth_obs, torch.Tensor):
            raise TypeError(f"After unpacking, depth_obs should be a Tensor, but got {type(depth_obs)}")
        
        if len(depth_obs.shape) != 3:
            raise ValueError(
                f"depth_obs should have 3 dimensions [batch, frames, height*width], "
                f"but got shape {depth_obs.shape}"
            )
        
        # Reshape: [batch, frames, height*width] -> [batch, frames, height, width]
        batch_size, num_frames, _ = depth_obs.shape
        expected_height, expected_width = self.vision_spatial_size
        depth_obs = depth_obs.view(batch_size, num_frames, expected_height, expected_width)
        
        vision_features = self.elevation_net(depth_obs)
        mean = self.fusion_actor(proprio_features, vision_features)
        return mean, self.extra_info

    def evaluate(self, obs: TensorDict, **kwargs: dict[str, Any]) -> torch.Tensor:
        obs = self.get_critic_obs(obs)
        obs = self.critic_obs_normalizer(obs)
        return self.critic(obs)

    def get_actor_obs(self, obs: TensorDict) -> torch.Tensor:
        """èŽ·å–actorçš„æœ¬ä½“è§‚æµ‹ï¼ˆæŽ’é™¤æ·±åº¦å›¾ï¼‰"""
        obs_list = []
        for obs_group in self.obs_groups["policy"]:
            # æ·±åº¦å›¾å•ç‹¬å¤„ç†ï¼Œä¸åŠ å…¥æœ¬ä½“è§‚æµ‹
            if obs_group != "height_scan_history":
                obs_list.append(obs[obs_group])
        return torch.cat(obs_list, dim=-1) if obs_list else torch.empty(obs[self.obs_groups["policy"][0]].shape[0], 0)

    def get_critic_obs(self, obs: TensorDict) -> torch.Tensor:
        """èŽ·å–criticè§‚æµ‹ï¼ˆæŽ’é™¤æ·±åº¦å›¾ï¼‰"""
        obs_list = []
        for obs_group in self.obs_groups["critic"]:
            # æ·±åº¦å›¾ä¸ç”¨äºŽcriticï¼ˆå¯ä»¥æ ¹æ®éœ€æ±‚ä¿®æ”¹ï¼‰
            if obs_group != "height_scan_history":
                obs_list.append(obs[obs_group])
        return torch.cat(obs_list, dim=-1) if obs_list else torch.empty(obs[self.obs_groups["critic"][0]].shape[0], 0)

    def get_actions_log_prob(self, actions: torch.Tensor) -> torch.Tensor:
        return self.distribution.log_prob(actions).sum(dim=-1)

    def update_normalization(self, obs: TensorDict) -> None:
        if self.actor_obs_normalization:
            actor_obs = self.get_actor_obs(obs)
            self.actor_obs_normalizer.update(actor_obs)
        if self.critic_obs_normalization:
            critic_obs = self.get_critic_obs(obs)
            self.critic_obs_normalizer.update(critic_obs)

    def load_state_dict(self, state_dict: dict, strict: bool = True) -> bool:
        """Load the parameters of the actor-critic model.

        Args:
            state_dict: State dictionary of the model.
            strict: Whether to strictly enforce that the keys in `state_dict` match the keys returned by this module's
                :meth:`state_dict` function.

        Returns:
            Whether this training resumes a previous training. This flag is used by the :func:`load` function of
                :class:`OnPolicyRunner` to determine how to load further parameters (relevant for, e.g., distillation).
        """
        super().load_state_dict(state_dict, strict=strict)
        return True

