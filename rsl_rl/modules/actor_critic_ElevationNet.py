# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

import torch
import torch.nn as nn
from tensordict import TensorDict
from torch.distributions import Normal
from typing import Any, NoReturn

from rsl_rl.networks import MLP, EmpiricalNormalization


class ActorCriticElevationNet(nn.Module):
    is_recurrent: bool = False

    def __init__(
        self,
        obs: TensorDict,
        obs_groups: dict[str, list[str]],
        num_actions: int,
        actor_obs_normalization: bool = False,
        critic_obs_normalization: bool = False,
        actor_hidden_dims: tuple[int] | list[int] = [256, 256, 256],
        critic_hidden_dims: tuple[int] | list[int] = [256, 256, 256],
        activation: str = "elu",
        init_noise_std: float = 1.0,
        noise_std_type: str = "scalar",
        state_dependent_std: bool = False,
        # ç½‘ç»œæž¶æž„æ¨¡å¼é€‰æ‹©
        network_mode: str = "mode1",  # "mode1", "mode2", "mode3"
        # é«˜ç¨‹å›¾MLPç¼–ç å™¨é…ç½®(Mode2/3ä½¿ç”¨)
        vision_feature_dim: int = 64,
        vision_num_frames: int = 5,
        vision_spatial_size: tuple[int, int] = (11, 11),
        # æœ¬ä½“MLPç¼–ç å™¨é…ç½®(Mode2/3ä½¿ç”¨)
        proprio_feature_dim: int = 128,
        # MLPèžåˆç½‘ç»œé…ç½®(Mode2/3ä½¿ç”¨)
        fusion_mlp_hidden_dims: list[int] | None = None,
        # Mode3ä¸“ç”¨å‚æ•°(ä¼°è®¡å™¨æ¨¡å¼)
        encoder_hidden_dims: tuple[int] | list[int] = [1024, 512, 256],
        decoder_hidden_dims: tuple[int] | list[int] = [256, 512, 1024],
        num_latent: int = 19,  # éšå‘é‡é•¿åº¦ï¼ŒåŒ…å«3ç»´çº¿é€Ÿåº¦
        num_decode: int = 30,  # è§£ç å™¨è¾“å‡ºç»´åº¦
        VAE_beta: float = 1.0,  # VAEçš„betaå‚æ•°
        **kwargs: dict[str, Any],
    ) -> None:
        super().__init__()

        # é…ç½®ç±»
        self.cfg = kwargs

        # ä¼ é€’å›žEnvçš„é¢å¤–ä¿¡æ¯
        self.extra_info = dict()

        # ä¿å­˜é…ç½®å‚æ•°
        self.obs_groups = obs_groups
        self.vision_spatial_size = vision_spatial_size
        self.state_dependent_std = state_dependent_std
        self.network_mode = network_mode
        self.beta = VAE_beta
        
        # è®¡ç®—æœ¬ä½“è§‚æµ‹å’Œcriticè§‚æµ‹ç»´åº¦
        num_actor_obs = sum(obs[g].shape[-1] for g in obs_groups["policy"] if g != "height_scan_history")
        num_critic_obs = sum(obs[g].shape[-1] for g in obs_groups["critic"] if g != "height_scan_history")
        
        # è®¡ç®—é«˜ç¨‹å›¾å±•å¹³åŽçš„ç»´åº¦
        height, width = vision_spatial_size
        height_map_dim = height * width

        ########################################## Actor ##############################################
        print("\n" + "=" * 80)
        print(f"ðŸŒŸ ç½‘ç»œæž¶æž„æ¨¡å¼: {network_mode}")
        print("=" * 80)
        
        if network_mode == "mode1":
            # Mode1: æœ¬ä½“è§‚æµ‹å’Œé«˜ç¨‹å›¾æ‹¼æŽ¥åŽè¿›å…¥ä¸€ä¸ªMLPç›´æŽ¥è¾“å‡ºaction
            print("âœ“ Mode1: æ‹¼æŽ¥ -> MLP -> Action")
            input_dim = num_actor_obs + height_map_dim
            self.direct_actor = MLP(input_dim, num_actions, actor_hidden_dims, activation)
            print(f"  Direct Actor: {input_dim} -> {num_actions}")
            
        elif network_mode == "mode2":
            # Mode2: æœ¬ä½“è§‚æµ‹å’Œé«˜ç¨‹å›¾åˆ†åˆ«è¿›MLPæå–ç‰¹å¾ï¼Œå†æ‹¼æŽ¥+MLPèžåˆåŽè¾“å‡ºaction
            print("âœ“ Mode2: æœ¬ä½“MLP -> ç‰¹å¾ + é«˜ç¨‹å›¾MLP -> ç‰¹å¾ + æ‹¼æŽ¥MLP -> Action")
            self.proprio_encoder = self._create_proprio_network(
                num_actor_obs, proprio_feature_dim, actor_hidden_dims, activation
            )
            self.elevation_net = self._create_perception_network(
                vision_feature_dim, vision_num_frames, vision_spatial_size
            )
            if fusion_mlp_hidden_dims is None:
                fusion_mlp_hidden_dims = [256, 128]
            self.fusion_actor = self._create_fusion_network(
                proprio_feature_dim, vision_feature_dim, num_actions, fusion_mlp_hidden_dims
            )
            height_map_input_dim = vision_num_frames * height_map_dim
            print(f"  Proprio MLP: {num_actor_obs} -> {proprio_feature_dim}")
            print(f"  Height Map MLP: {height_map_input_dim} ({vision_num_frames}Ã—{height_map_dim}) -> {vision_feature_dim}")
            print(f"  Fusion MLP: {proprio_feature_dim + vision_feature_dim} -> {num_actions}")
            
        elif network_mode == "mode3":
            # Mode3: ç±»ä¼¼Mode2ï¼Œä½†è¾“å‡ºéšå‘é‡(åŒ…æ‹¬é€Ÿåº¦ä¼°è®¡vå’Œçº¯éšå‘é‡z)ï¼Œç±»ä¼¼DWAQ
            print("âœ“ Mode3: æœ¬ä½“MLP + é«˜ç¨‹å›¾MLP -> æ‹¼æŽ¥MLP -> éšå‘é‡(v+z) -> Encoder/Decoder")
            self.proprio_encoder = self._create_proprio_network(
                num_actor_obs, proprio_feature_dim, actor_hidden_dims, activation
            )
            self.elevation_net = self._create_perception_network(
                vision_feature_dim, vision_num_frames, vision_spatial_size
            )
            
            # èžåˆç½‘ç»œè¾“å‡ºéšå‘é‡ç‰¹å¾
            fusion_output_dim = encoder_hidden_dims[-1]
            if fusion_mlp_hidden_dims is None:
                fusion_mlp_hidden_dims = [256, 128]
            self.fusion_encoder = self._create_fusion_network(
                proprio_feature_dim, vision_feature_dim, fusion_output_dim, fusion_mlp_hidden_dims
            )
            
            # Encoderåˆ†æ”¯ï¼šè¾“å‡ºå‡å€¼å’Œæ–¹å·®
            self.encoder_latent_mean = nn.Linear(fusion_output_dim, num_latent - 3)
            self.encoder_latent_logvar = nn.Linear(fusion_output_dim, num_latent - 3)
            self.encoder_vel_mean = nn.Linear(fusion_output_dim, 3)
            self.encoder_vel_logvar = nn.Linear(fusion_output_dim, 3)
            
            # Decoderï¼šä»Žéšå‘é‡é‡å»ºè§‚æµ‹
            self.decoder = MLP(num_latent, num_decode, decoder_hidden_dims, activation)
            
            # Actorï¼šä»Žéšå‘é‡+å½“å‰æœ¬ä½“è§‚æµ‹è¾“å‡ºåŠ¨ä½œ
            self.num_decode = num_decode
            self.actor = MLP(num_latent + num_actor_obs, num_actions, actor_hidden_dims, activation)
            
            height_map_input_dim = vision_num_frames * height_map_dim
            print(f"  Proprio MLP: {num_actor_obs} -> {proprio_feature_dim}")
            print(f"  Height Map MLP: {height_map_input_dim} ({vision_num_frames}Ã—{height_map_dim}) -> {vision_feature_dim}")
            print(f"  Fusion MLP: {proprio_feature_dim + vision_feature_dim} -> {fusion_output_dim}")
            print(f"  Latent: {num_latent} (vel: 3, latent: {num_latent-3})")
            print(f"  Decoder: {num_latent} -> {num_decode}")
            print(f"  Actor: {num_latent + num_actor_obs} -> {num_actions}")
        else:
            raise ValueError(f"Unknown network mode: {network_mode}. Should be 'mode1', 'mode2', or 'mode3'")
        
            print("=" * 80 + "\n")

        # Actor observation normalization
        self.actor_obs_normalization = actor_obs_normalization
        if actor_obs_normalization:
            self.actor_obs_normalizer = EmpiricalNormalization(num_actor_obs)
        else:
            self.actor_obs_normalizer = torch.nn.Identity()


        ########################################## Critic ##############################################
        self.critic = MLP(num_critic_obs, 1, critic_hidden_dims, activation)
        print(f"Critic MLP: {self.critic}")

        # Critic observation normalization
        self.critic_obs_normalization = critic_obs_normalization
        if critic_obs_normalization:
            self.critic_obs_normalizer = EmpiricalNormalization(num_critic_obs)
        else:
            self.critic_obs_normalizer = torch.nn.Identity()

        # Action noise
        self.noise_std_type = noise_std_type
        # Transformer Fusionæž¶æž„ä¸æ”¯æŒstate_dependent_std(å·²åœ¨å‰é¢éªŒè¯)
        if self.noise_std_type == "scalar":
            self.std = nn.Parameter(init_noise_std * torch.ones(num_actions))
        elif self.noise_std_type == "log":
            self.log_std = nn.Parameter(torch.log(init_noise_std * torch.ones(num_actions)))
        else:
            raise ValueError(f"Unknown standard deviation type: {self.noise_std_type}. Should be 'scalar' or 'log'")

        # Action distribution
        # Note: Populated in update_distribution
        self.distribution = None

        # Disable args validation for speedup
        Normal.set_default_validate_args(False)

    def _create_proprio_network(
        self, 
        num_actor_obs: int, 
        proprio_feature_dim: int, 
        actor_hidden_dims: list[int], 
        activation: str
    ) -> nn.Module:
        """åˆ›å»ºæœ¬ä½“ä¿¡æ¯ç¼–ç ç½‘ç»œ
        
        Mode2: ä½¿ç”¨ç®€å•MLPæå–æœ¬ä½“ç‰¹å¾
        Mode3: åŒMode2
        
        Args:
            num_actor_obs: æœ¬ä½“è§‚æµ‹ç»´åº¦
            proprio_feature_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            actor_hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
            activation: æ¿€æ´»å‡½æ•°åç§°
            
        Returns:
            æœ¬ä½“ç¼–ç å™¨ç½‘ç»œ
        """
        return MLP(num_actor_obs, proprio_feature_dim, actor_hidden_dims, activation)
    
    def _create_perception_network(
        self,
        vision_feature_dim: int,
        vision_num_frames: int,
        vision_spatial_size: tuple[int, int]
    ) -> nn.Module:
        """åˆ›å»ºæ„ŸçŸ¥ç½‘ç»œ(å¤„ç†é«˜ç¨‹å›¾åºåˆ—)
        
        Mode2/3: ä½¿ç”¨MLPå¤„ç†å±•å¹³çš„é«˜ç¨‹å›¾åºåˆ—
        
        Args:
            vision_feature_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            vision_num_frames: å¸§æ•°
            vision_spatial_size: ç©ºé—´å°ºå¯¸ (height, width)
            
        Returns:
            æ„ŸçŸ¥ç¼–ç å™¨ç½‘ç»œ
        """
        # é«˜ç¨‹å›¾å±•å¹³åŽçš„ç»´åº¦ = frames * height * width
        height, width = vision_spatial_size
        input_dim = vision_num_frames * height * width
        
        # ä½¿ç”¨MLPå¤„ç†å±•å¹³çš„é«˜ç¨‹å›¾åºåˆ—
        # éšè—å±‚ç»´åº¦æ ¹æ®è¾“å…¥è¾“å‡ºè‡ªé€‚åº”è®¾ç½®
        hidden_dims = [max(input_dim // 2, vision_feature_dim * 2), vision_feature_dim * 2]
        return MLP(input_dim, vision_feature_dim, hidden_dims, "elu")
    
    def _create_fusion_network(
        self,
        proprio_feature_dim: int,
        vision_feature_dim: int,
        num_actions: int,
        fusion_mlp_hidden_dims: list[int]
    ) -> nn.Module:
        """åˆ›å»ºèžåˆç½‘ç»œ(æ‹¼æŽ¥æœ¬ä½“å’Œæ„ŸçŸ¥ç‰¹å¾å¹¶è¾“å‡ºåŠ¨ä½œ/éšå‘é‡)
        
        Mode2/3: ä½¿ç”¨ç®€å•æ‹¼æŽ¥+MLPèžåˆä¸¤ä¸ªç‰¹å¾
        
        Args:
            proprio_feature_dim: æœ¬ä½“ç‰¹å¾ç»´åº¦
            vision_feature_dim: è§†è§‰ç‰¹å¾ç»´åº¦
            num_actions: è¾“å‡ºç»´åº¦(Mode2ä¸ºåŠ¨ä½œç»´åº¦ï¼ŒMode3ä¸ºéšå‘é‡ç»´åº¦)
            fusion_mlp_hidden_dims: MLPçš„éšè—å±‚ç»´åº¦
            
        Returns:
            èžåˆç½‘ç»œ
        """
        # ç®€å•æ‹¼æŽ¥ä¸¤ä¸ªç‰¹å¾åŽç”¨MLPè¾“å‡º
        input_dim = proprio_feature_dim + vision_feature_dim
        return MLP(input_dim, num_actions, fusion_mlp_hidden_dims, "elu")

    def reset(self, dones: torch.Tensor | None = None) -> None:
        pass

    def forward(self) -> NoReturn:
        raise NotImplementedError

    @property
    def action_mean(self) -> torch.Tensor:
        return self.distribution.mean

    @property
    def action_std(self) -> torch.Tensor:
        return self.distribution.stddev

    @property
    def entropy(self) -> torch.Tensor:
        return self.distribution.entropy().sum(dim=-1)

    def _extract_height_map(self, obs: TensorDict) -> torch.Tensor:
        """æå–é«˜ç¨‹å›¾çš„å•å¸§å±•å¹³æ•°æ®(ç”¨äºŽmode1)"""
        depth_obs = obs["height_scan_history"]
        while isinstance(depth_obs, TensorDict):
            keys = list(depth_obs.keys())
            depth_obs = depth_obs[keys[0]]
        # å–æœ€æ–°ä¸€å¸§: [batch, frames, height*width] -> [batch, height*width]
        return depth_obs[:, -1, :]
    
    def _extract_height_map_sequence(self, obs: TensorDict) -> torch.Tensor:
        """æå–é«˜ç¨‹å›¾åºåˆ—å¹¶å±•å¹³(ç”¨äºŽmode2çš„MLP)"""
        depth_obs = obs["height_scan_history"]
        while isinstance(depth_obs, TensorDict):
            keys = list(depth_obs.keys())
            depth_obs = depth_obs[keys[0]]
        # å±•å¹³æ‰€æœ‰å¸§: [batch, frames, height*width] -> [batch, frames*height*width]
        batch_size = depth_obs.shape[0]
        return depth_obs.view(batch_size, -1)

    def reparameterise(self, mean, logvar):
        """é‡å‚æ•°åŒ–æŠ€å·§(ç”¨äºŽmode3çš„VAE)"""
        std = torch.exp(logvar * 0.5)
        code_temp = torch.randn_like(std)
        code = mean + std * code_temp
        return code
    
    def encoder_forward(self, proprio_obs: torch.Tensor, obs: TensorDict):
        """Mode3ç¼–ç å™¨å‰å‘æŽ¨ç†"""
        # 1. æå–æœ¬ä½“ç‰¹å¾
        proprio_features = self.proprio_encoder(proprio_obs)
        
        # 2. æå–å¹¶å¤„ç†é«˜ç¨‹å›¾åºåˆ—(ä½¿ç”¨MLP)
        height_map_sequence = self._extract_height_map_sequence(obs)
        vision_features = self.elevation_net(height_map_sequence)
        
        # 3. æ‹¼æŽ¥ä¸¤ä¸ªç‰¹å¾åŽèžåˆå¾—åˆ°ç¼–ç ç‰¹å¾
        fused_features = torch.cat([proprio_features, vision_features], dim=-1)
        x = self.fusion_encoder(fused_features)
        
        # 4. åˆ†åˆ«è¾“å‡ºé€Ÿåº¦å’Œéšå‘é‡çš„å‡å€¼å’Œæ–¹å·®
        latent_mean = self.encoder_latent_mean(x)
        latent_logvar = self.encoder_latent_logvar(x)
        vel_mean = self.encoder_vel_mean(x)
        vel_logvar = self.encoder_vel_logvar(x)
        
        # é™åˆ¶æ–¹å·®èŒƒå›´
        latent_logvar = torch.clip(latent_logvar, min=-10, max=10)
        vel_logvar = torch.clip(vel_logvar, min=-10, max=10)
        
        # é‡‡æ ·
        latent_sample = self.reparameterise(latent_mean, latent_logvar)
        vel_sample = self.reparameterise(vel_mean, vel_logvar)
        
        # æ‹¼æŽ¥æˆå®Œæ•´éšå‘é‡
        code = torch.cat((vel_sample, latent_sample), dim=-1)
        
        # è§£ç 
        decode = self.decoder(code)
        
        return code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar

    def _update_distribution(self, obs_input: torch.Tensor) -> None:
        """æ›´æ–°åŠ¨ä½œåˆ†å¸ƒ(ç»Ÿä¸€æŽ¥å£ï¼Œæ ¹æ®modeå†³å®šè¾“å…¥)"""
        mean = obs_input  # å¯¹äºŽmode1å’Œmode2ï¼Œobs_inputå·²ç»æ˜¯action mean
        
        # è®¡ç®—æ ‡å‡†å·®
        if self.noise_std_type == "scalar":
            std = self.std.expand_as(mean)
        elif self.noise_std_type == "log":
            std = torch.exp(self.log_std).expand_as(mean)
        
        # åˆ›å»ºåˆ†å¸ƒ
        self.distribution = Normal(mean, std)

    def act(self, obs: TensorDict, **kwargs: dict[str, Any]) -> torch.Tensor:
        proprio_obs = self.get_actor_obs(obs)
        proprio_obs = self.actor_obs_normalizer(proprio_obs)
        
        if self.network_mode == "mode1":
            # Mode1: æ‹¼æŽ¥æœ¬ä½“è§‚æµ‹å’Œé«˜ç¨‹å›¾å•å¸§ï¼Œç›´æŽ¥è¾“å…¥MLP
            height_map = self._extract_height_map(obs)
            actor_input = torch.cat([proprio_obs, height_map], dim=-1)
            mean = self.direct_actor(actor_input)
            self._update_distribution(mean)
            
        elif self.network_mode == "mode2":
            # Mode2: æœ¬ä½“å’Œé«˜ç¨‹å›¾åºåˆ—åˆ†åˆ«æå–ç‰¹å¾ï¼Œç„¶åŽæ‹¼æŽ¥åŽèžåˆ
            proprio_features = self.proprio_encoder(proprio_obs)
            height_map = self._extract_height_map(obs)
            vision_features = self.elevation_net(height_map)
            # æ‹¼æŽ¥ä¸¤ä¸ªç‰¹å¾
            fused_features = torch.cat([proprio_features, vision_features], dim=-1)
            mean = self.fusion_actor(fused_features)
            self._update_distribution(mean)
            
        elif self.network_mode == "mode3":
            # Mode3: ç¼–ç å™¨è¾“å‡ºéšå‘é‡ï¼Œä¸Žå½“å‰è§‚æµ‹æ‹¼æŽ¥åŽè¾“å…¥actor
            code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar = \
                self.encoder_forward(proprio_obs, obs)
            
            # å°†codeå’Œå½“å‰æœ¬ä½“è§‚æµ‹æ‹¼æŽ¥
            observation = torch.cat((code.detach(), proprio_obs), dim=-1)
            mean = self.actor(observation)
            self._update_distribution(mean)
            
            # è®°å½•é¢å¤–ä¿¡æ¯ç”¨äºŽç›‘æŽ§
            self.extra_info["est_vel"] = vel_mean
            self.extra_info["obs_predict"] = decode * (self.actor_obs_normalizer.std[:self.num_decode] + 1e-2) + \
                                                       self.actor_obs_normalizer.mean[:self.num_decode]
        
        return self.distribution.sample(), self.extra_info

    def act_inference(self, obs: TensorDict) -> torch.Tensor:
        proprio_obs = self.get_actor_obs(obs)
        proprio_obs = self.actor_obs_normalizer(proprio_obs)
        
        if self.network_mode == "mode1":
            # Mode1: æ‹¼æŽ¥åŽç›´æŽ¥æŽ¨ç†
            height_map = self._extract_height_map(obs)
            actor_input = torch.cat([proprio_obs, height_map], dim=-1)
            mean = self.direct_actor(actor_input)
            
        elif self.network_mode == "mode2":
            # Mode2: åˆ†åˆ«æå–ç‰¹å¾åŽæ‹¼æŽ¥èžåˆ
            proprio_features = self.proprio_encoder(proprio_obs)
            height_map_sequence = self._extract_height_map_sequence(obs)
            vision_features = self.elevation_net(height_map_sequence)
            fused_features = torch.cat([proprio_features, vision_features], dim=-1)
            mean = self.fusion_actor(fused_features)
            
        elif self.network_mode == "mode3":
            # Mode3: ä½¿ç”¨å‡å€¼è€Œéžé‡‡æ ·å€¼
            code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar = \
                self.encoder_forward(proprio_obs, obs)
        
            # æŽ¨ç†æ—¶ä½¿ç”¨å‡å€¼
            observation = torch.cat((vel_mean.detach(), latent_mean.detach(), proprio_obs), dim=-1)
            mean = self.actor(observation)
            
            # è®°å½•é¢å¤–ä¿¡æ¯
            self.extra_info["est_vel"] = vel_mean
            self.extra_info["obs_predict"] = decode * (self.actor_obs_normalizer.std[:self.num_decode] + 1e-2) + \
                                            self.actor_obs_normalizer.mean[:self.num_decode]
        
        return mean, self.extra_info

    def evaluate(self, obs: TensorDict, **kwargs: dict[str, Any]) -> torch.Tensor:
        obs = self.get_critic_obs(obs)
        obs = self.critic_obs_normalizer(obs)
        return self.critic(obs)

    def get_actor_obs(self, obs: TensorDict) -> torch.Tensor:
        """èŽ·å–actorçš„æœ¬ä½“è§‚æµ‹(æŽ’é™¤æ·±åº¦å›¾)"""
        obs_list = []
        for obs_group in self.obs_groups["policy"]:
            # æ·±åº¦å›¾å•ç‹¬å¤„ç†ï¼Œä¸åŠ å…¥æœ¬ä½“è§‚æµ‹
            if obs_group != "height_scan_history":
                obs_list.append(obs[obs_group])
        return torch.cat(obs_list, dim=-1) if obs_list else torch.empty(obs[self.obs_groups["policy"][0]].shape[0], 0)

    def get_critic_obs(self, obs: TensorDict) -> torch.Tensor:
        """èŽ·å–criticè§‚æµ‹(æŽ’é™¤æ·±åº¦å›¾)"""
        obs_list = []
        for obs_group in self.obs_groups["critic"]:
            # æ·±åº¦å›¾ä¸ç”¨äºŽcritic(å¯ä»¥æ ¹æ®éœ€æ±‚ä¿®æ”¹)
            if obs_group != "height_scan_history":
                obs_list.append(obs[obs_group])
        return torch.cat(obs_list, dim=-1) if obs_list else torch.empty(obs[self.obs_groups["critic"][0]].shape[0], 0)

    def get_actions_log_prob(self, actions: torch.Tensor) -> torch.Tensor:
        return self.distribution.log_prob(actions).sum(dim=-1)

    def update_normalization(self, obs: TensorDict) -> None:
        if self.actor_obs_normalization:
            actor_obs = self.get_actor_obs(obs)
            self.actor_obs_normalizer.update(actor_obs)
        if self.critic_obs_normalization:
            critic_obs = self.get_critic_obs(obs)
            self.critic_obs_normalizer.update(critic_obs)

    def update_encoder(
        self,
        obs_batch: TensorDict,
        next_observations_batch: TensorDict,
        encoder_optimizer: torch.optim.Optimizer,
        max_grad_norm: float
    ) -> dict[str, float]:
        """æ›´æ–°Mode3çš„ç¼–ç å™¨(ä»…åœ¨mode3ä¸‹ä½¿ç”¨)

        Args:
            obs_batch: å½“å‰è§‚æµ‹æ‰¹æ¬¡æ•°æ®
            next_observations_batch: ä¸‹ä¸€æ—¶åˆ»è§‚æµ‹æ‰¹æ¬¡æ•°æ®
            encoder_optimizer: ç¼–ç å™¨ä¼˜åŒ–å™¨
            max_grad_norm: æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°

        Returns:
            æŸå¤±å­—å…¸ï¼ŒåŒ…å«å„é¡¹æŸå¤±å€¼
        """
        if self.network_mode != "mode3":
            return {}

        # èŽ·å–å¹¶å½’ä¸€åŒ–policyè§‚æµ‹
        policy_obs = self.get_actor_obs(obs_batch)
        policy_obs = self.actor_obs_normalizer(policy_obs)

        # å‰å‘ä¼ æ’­å¾—åˆ°ç¼–ç å™¨è¾“å‡º
        code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar = \
            self.encoder_forward(policy_obs, obs_batch)

        # èŽ·å–å¹¶å½’ä¸€åŒ–criticè§‚æµ‹ï¼Œæå–çœŸå®žé€Ÿåº¦
        critic_obs = self.get_critic_obs(obs_batch)
        critic_obs = self.critic_obs_normalizer(critic_obs)
        vel_target = critic_obs[:, 0:3]  # çœŸå®žé€Ÿåº¦ä½œä¸ºç›®æ ‡

        # èŽ·å–ä¸‹ä¸€æ—¶åˆ»è§‚æµ‹ï¼Œæå–ç›®æ ‡è§‚æµ‹
        next_observations = self.get_actor_obs(next_observations_batch)
        next_observations = self.actor_obs_normalizer(next_observations)
        obs_target = next_observations[:, 0:self.num_decode]  # å–æœ€æ–°è§‚æµ‹

        vel_target.requires_grad = False
        obs_target.requires_grad = False

        # æŸå¤±è®¡ç®—ï¼šé€Ÿåº¦é‡å»ºæŸå¤± + obsé‡å»ºæŸå¤± + KLæ•£åº¦æŸå¤±
        vel_MSE = nn.MSELoss()(vel_sample, vel_target) * 100.0
        obs_MSE = nn.MSELoss()(decode, obs_target)
        dkl_loss = -0.5 * torch.mean(torch.sum(1 + latent_logvar - latent_mean.pow(2) - latent_logvar.exp(), dim=1))
        autoenc_loss = vel_MSE + obs_MSE + self.beta * dkl_loss

        # åå‘ä¼ æ’­
        encoder_optimizer.zero_grad()
        autoenc_loss.backward(retain_graph=True)

        # æ¢¯åº¦è£å‰ª
        encoder_params = [p for group in encoder_optimizer.param_groups for p in group['params']]
        nn.utils.clip_grad_norm_(encoder_params, max_grad_norm)

        # æ›´æ–°å‚æ•°
        encoder_optimizer.step()

        return {
            "vel_loss": vel_MSE.item(),
            "obs_loss": obs_MSE.item(),
            "dkl_loss": dkl_loss.item(),
            "total_loss": autoenc_loss.item(),
        }

    def load_state_dict(self, state_dict: dict, strict: bool = True) -> bool:
        """Load the parameters of the actor-critic model.

        Args:
            state_dict: State dictionary of the model.
            strict: Whether to strictly enforce that the keys in `state_dict` match the keys returned by this module's
                :meth:`state_dict` function.

        Returns:
            Whether this training resumes a previous training. This flag is used by the :func:`load` function of
                :class:`OnPolicyRunner` to determine how to load further parameters (relevant for, e.g., distillation).
        """
        super().load_state_dict(state_dict, strict=strict)
        return True
