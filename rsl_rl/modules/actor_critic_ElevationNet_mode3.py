# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
ActorCriticElevationNetMode3: VAEç¼–ç å™¨æ¶æ„

ç½‘ç»œç»“æ„:
    æœ¬ä½“ -> æœ¬ä½“ç¼–ç å™¨MLP -> ç‰¹å¾1
    é«˜ç¨‹å›¾ -> é«˜ç¨‹å›¾ç¼–ç å™¨MLP -> ç‰¹å¾2
    [ç‰¹å¾1 + ç‰¹å¾2] -> èåˆMLP -> ç¼–ç ç‰¹å¾
    ç¼–ç ç‰¹å¾ -> Encoder -> éšå‘é‡(v+z)
    éšå‘é‡ -> Decoder -> é‡å»ºè§‚æµ‹
    [éšå‘é‡ + æœ¬ä½“] -> Actor MLP -> åŠ¨ä½œ
"""

from __future__ import annotations

import torch
import torch.nn as nn
from tensordict import TensorDict
from torch.distributions import Normal
from typing import Any, NoReturn

from rsl_rl.networks import MLP, EmpiricalNormalization
import copy
import os

class ActorCriticElevationNetMode3(nn.Module):
    """Mode3: ç±»ä¼¼Mode2ä½†è¾“å‡ºéšå‘é‡ï¼ˆåŒ…å«é€Ÿåº¦ä¼°è®¡ï¼‰ï¼Œç±»ä¼¼DWAQ
    
    ç½‘ç»œç»„æˆ:
    1. æœ¬ä½“ç¼–ç å™¨MLP - æå–æœ¬ä½“ç‰¹å¾
    2. é«˜ç¨‹å›¾ç¼–ç å™¨MLP - æå–è§†è§‰ç‰¹å¾
    3. èåˆMLP - èåˆç‰¹å¾
    4. VAE Encoder - è¾“å‡ºéšå‘é‡(é€Ÿåº¦v + éšçŠ¶æ€z)
    5. VAE Decoder - é‡å»ºè§‚æµ‹
    6. Actor MLP - ä»éšå‘é‡+æœ¬ä½“è§‚æµ‹è¾“å‡ºåŠ¨ä½œ
    7. Critic MLP - ä»·å€¼è¯„ä¼°
    """
    
    is_recurrent: bool = False

    def __init__(
        self,
        obs: TensorDict,
        obs_groups: dict[str, list[str]],
        num_actions: int,
        env_cfg=None,
        alg_cfg: dict | None = None,
        actor_obs_normalization: bool = False,
        critic_obs_normalization: bool = False,
        actor_hidden_dims: tuple[int] | list[int] = [128, 64],
        critic_hidden_dims: tuple[int] | list[int] = [256, 128, 64],
        activation: str = "elu",
        init_noise_std: float = 1.0,
        noise_std_type: str = "scalar",
        # é«˜ç¨‹å›¾ç¼–ç å™¨é…ç½®
        vision_feature_dim: int = 32,
        history_frames: int = 5,
        vision_spatial_size: tuple[int, int] = (25, 17),
        elevation_encoder_hidden_dims: list[int] | None = None,
        # æœ¬ä½“ç¼–ç å™¨é…ç½®
        proprio_feature_dim: int = 64,
        proprio_encoder_hidden_dims: list[int] | None = None,
        # èåˆç½‘ç»œé…ç½®
        fusion_actor_hidden_dims: list[int] | None = None,
        # VAEç¼–ç å™¨-è§£ç å™¨é…ç½®
        encoder_hidden_dims: tuple[int] | list[int] = [1024, 512, 256],
        decoder_hidden_dims: tuple[int] | list[int] = [256, 512, 1024],
        num_latent: int = 19,
        num_decode: int = 30,
        VAE_beta: float = 1.0,
        **kwargs: dict[str, Any],
    ) -> None:
        super().__init__()

        # é…ç½®
        self.cfg = kwargs
        self.extra_info = dict()
        self.obs_groups = obs_groups
        self.vision_spatial_size = vision_spatial_size
        self.noise_std_type = noise_std_type
        self.beta = VAE_beta
        self.num_decode = num_decode
        num_actor_obs = 0
        
        
        # è®¡ç®—è§‚æµ‹ç»´åº¦
        num_actor_obs = sum(obs[g].shape[-1] for g in obs_groups["policy"] if g != "height_scan_history")
        num_critic_obs = sum(obs[g].shape[-1] for g in obs_groups["critic"] if g != "height_scan_history")
        self.obs_one_frame_len: int = int(num_actor_obs / history_frames)
        
        # è®¡ç®—é«˜ç¨‹å›¾å±•å¹³åçš„ç»´åº¦
        height, width = vision_spatial_size
        height_map_dim = height * width
        height_map_input_dim = history_frames * height_map_dim
        
        ########################################## Actor ##############################################
        print("\n" + "=" * 80)
        print("ğŸŒŸ ç½‘ç»œæ¶æ„: ElevationNet Mode3 (VAE)")
        print("=" * 80)
        print("âœ“ Mode3: æœ¬ä½“ç¼–ç å™¨ + é«˜ç¨‹å›¾ç¼–ç å™¨ + èåˆç½‘ç»œ + VAE -> åŠ¨ä½œ")
        
        # 1. æœ¬ä½“ç¼–ç å™¨MLP
        if proprio_encoder_hidden_dims is None:
            proprio_encoder_hidden_dims = actor_hidden_dims
        self.proprio_encoder = MLP(num_actor_obs, proprio_feature_dim, proprio_encoder_hidden_dims, activation)
        print(f"  1. æœ¬ä½“ç¼–ç å™¨: {num_actor_obs} -> {proprio_encoder_hidden_dims} -> {proprio_feature_dim}")
        
        # 2. é«˜ç¨‹å›¾ç¼–ç å™¨MLP
        if elevation_encoder_hidden_dims is None:
            elevation_encoder_hidden_dims = [max(height_map_input_dim // 2, vision_feature_dim * 2), vision_feature_dim * 2]
        self.elevation_net = MLP(height_map_input_dim, vision_feature_dim, elevation_encoder_hidden_dims, "elu")
        print(f"  2. é«˜ç¨‹å›¾ç¼–ç å™¨: {height_map_input_dim} ({history_frames}Ã—{height_map_dim}) -> {elevation_encoder_hidden_dims} -> {vision_feature_dim}")
        
        # 3. èåˆMLP
        fusion_output_dim = encoder_hidden_dims[-1]
        if fusion_actor_hidden_dims is None:
            fusion_actor_hidden_dims = actor_hidden_dims if actor_hidden_dims else [256, 128]
            print(f"     â„¹ï¸  fusion_actor_hidden_dimsæœªè®¾ç½®ï¼Œä½¿ç”¨actor_hidden_dims={fusion_actor_hidden_dims}")
        fusion_input_dim = proprio_feature_dim + vision_feature_dim
        self.fusion_encoder = MLP(fusion_input_dim, fusion_output_dim, fusion_actor_hidden_dims, "elu")
        print(f"  3. èåˆMLP: {fusion_input_dim} (æœ¬ä½“{proprio_feature_dim} + è§†è§‰{vision_feature_dim}) -> {fusion_actor_hidden_dims} -> {fusion_output_dim}")
        
        # 4. VAE Encoder: è¾“å‡ºå‡å€¼å’Œæ–¹å·®
        self.encoder_latent_mean = nn.Linear(fusion_output_dim, num_latent - 3)
        self.encoder_latent_logvar = nn.Linear(fusion_output_dim, num_latent - 3)
        self.encoder_vel_mean = nn.Linear(fusion_output_dim, 3)
        self.encoder_vel_logvar = nn.Linear(fusion_output_dim, 3)
        print(f"  4. VAE Encoder: {fusion_output_dim} -> éšå‘é‡{num_latent} (é€Ÿåº¦3 + éšçŠ¶æ€{num_latent-3})")
        
        # 5. VAE Decoder: ä»éšå‘é‡é‡å»ºè§‚æµ‹
        self.decoder = MLP(num_latent, num_decode, decoder_hidden_dims, activation)
        print(f"  5. VAE Decoder: {num_latent} -> {decoder_hidden_dims} -> {num_decode}")
        
        # 6. Actor: ä»éšå‘é‡+å½“å‰æœ¬ä½“è§‚æµ‹è¾“å‡ºåŠ¨ä½œ
        actor_input_dim = num_latent + self.obs_one_frame_len
        self.actor = MLP(actor_input_dim, num_actions, actor_hidden_dims, activation)
        print(f"  6. Actor MLP: {actor_input_dim} (éšå‘é‡{num_latent} + æœ¬ä½“{self.obs_one_frame_len}) -> {actor_hidden_dims} -> {num_actions}")

        # Actor observation normalization
        self.actor_obs_normalization = actor_obs_normalization
        if actor_obs_normalization:
            self.actor_obs_normalizer = EmpiricalNormalization(num_actor_obs)
        else:
            self.actor_obs_normalizer = torch.nn.Identity()

        ########################################## Critic ##############################################
        self.critic = MLP(num_critic_obs, 1, critic_hidden_dims, activation)
        print(f"  7. Critic MLP: {num_critic_obs} -> {critic_hidden_dims} -> 1")
        print(f"\n  VAE Beta: {VAE_beta}")
        print("=" * 80 + "\n")

        # Critic observation normalization
        self.critic_obs_normalization = critic_obs_normalization
        if critic_obs_normalization:
            self.critic_obs_normalizer = EmpiricalNormalization(num_critic_obs)
        else:
            self.critic_obs_normalizer = torch.nn.Identity()

        ########################################## Action Noise ##############################################
        if self.noise_std_type == "scalar":
            self.std = nn.Parameter(init_noise_std * torch.ones(num_actions))
        elif self.noise_std_type == "log":
            self.log_std = nn.Parameter(torch.log(init_noise_std * torch.ones(num_actions)))
        else:
            raise ValueError(f"Unknown standard deviation type: {self.noise_std_type}")

        # Action distribution
        self.distribution = None
        Normal.set_default_validate_args(False)

    def reset(self, dones: torch.Tensor | None = None) -> None:
        pass

    def forward(self) -> NoReturn:
        raise NotImplementedError

    @property
    def action_mean(self) -> torch.Tensor:
        return self.distribution.mean

    @property
    def action_std(self) -> torch.Tensor:
        return self.distribution.stddev

    @property
    def entropy(self) -> torch.Tensor:
        return self.distribution.entropy().sum(dim=-1)

    def _extract_height_map_sequence(self, obs: TensorDict) -> torch.Tensor:
        """æå–é«˜ç¨‹å›¾åºåˆ—å¹¶å±•å¹³"""
        depth_obs = obs["height_scan_history"]
        while isinstance(depth_obs, TensorDict):
            keys = list(depth_obs.keys())
            depth_obs = depth_obs[keys[0]]
        # å±•å¹³æ‰€æœ‰å¸§: [batch, frames, height*width] -> [batch, frames*height*width]
        batch_size = depth_obs.shape[0]
        return depth_obs.view(batch_size, -1)

    def reparameterise(self, mean: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        """é‡å‚æ•°åŒ–æŠ€å·§"""
        std = torch.exp(logvar * 0.5)
        code_temp = torch.randn_like(std)
        code = mean + std * code_temp
        return code
    
    def encoder_forward(self, proprio_obs: torch.Tensor, obs: TensorDict):
        """ç¼–ç å™¨å‰å‘ä¼ æ’­"""
        # 1. æå–æœ¬ä½“ç‰¹å¾
        proprio_features = self.proprio_encoder(proprio_obs)
        
        # 2. æå–é«˜ç¨‹å›¾ç‰¹å¾
        height_map_sequence = self._extract_height_map_sequence(obs) # TODO: é«˜ç¨‹å›¾æ˜¯å¦éœ€è¦å½’ä¸€åŒ–
        vision_features = self.elevation_net(height_map_sequence)
        
        # 3. èåˆç‰¹å¾
        fused_features = torch.cat([proprio_features, vision_features], dim=-1)
        x = self.fusion_encoder(fused_features)
        
        # 4. VAEç¼–ç : è¾“å‡ºå‡å€¼å’Œæ–¹å·®
        latent_mean = self.encoder_latent_mean(x)
        latent_logvar = self.encoder_latent_logvar(x)
        vel_mean = self.encoder_vel_mean(x)
        vel_logvar = self.encoder_vel_logvar(x)
        
        # é™åˆ¶æ–¹å·®èŒƒå›´
        latent_logvar = torch.clip(latent_logvar, min=-10, max=10)
        vel_logvar = torch.clip(vel_logvar, min=-10, max=10)
        
        # 5. é‡‡æ ·éšå‘é‡
        latent_sample = self.reparameterise(latent_mean, latent_logvar)
        vel_sample = self.reparameterise(vel_mean, vel_logvar)
        
        # 6. æ‹¼æ¥æˆå®Œæ•´éšå‘é‡
        code = torch.cat((vel_sample, latent_sample), dim=-1)
        
        # 7. è§£ç 
        decode = self.decoder(code)
        
        return code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar

    def _update_distribution(self, mean: torch.Tensor) -> None:
        """æ›´æ–°åŠ¨ä½œåˆ†å¸ƒ"""
        if self.noise_std_type == "scalar":
            std = self.std.expand_as(mean)
        elif self.noise_std_type == "log":
            std = torch.exp(self.log_std).expand_as(mean)
        
        self.distribution = Normal(mean, std)

    def act(self, obs: TensorDict, **kwargs: dict[str, Any]) -> tuple[torch.Tensor, dict]:
        """è®­ç»ƒæ—¶çš„åŠ¨ä½œé‡‡æ ·"""
        # 1. è·å–å¹¶å½’ä¸€åŒ–æœ¬ä½“è§‚æµ‹
        proprio_obs = self.get_actor_obs(obs)
        proprio_obs = self.actor_obs_normalizer(proprio_obs)
        
        # 2. ç¼–ç å™¨å‰å‘ä¼ æ’­å¾—åˆ°éšå‘é‡
        code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar = \
            self.encoder_forward(proprio_obs, obs)
        
        # 3. å°†éšå‘é‡ä¸å½“å‰æœ¬ä½“è§‚æµ‹æ‹¼æ¥
        now_obs = proprio_obs[:, 0:self.obs_one_frame_len]  # å–å½“å‰è§‚æµ‹å€¼éƒ¨åˆ†
        observation = torch.cat((code.detach(), now_obs), dim=-1)
        
        # 4. Actorè¾“å‡ºåŠ¨ä½œ
        mean = self.actor(observation)
        self._update_distribution(mean)
        
        # 5. è®°å½•é¢å¤–ä¿¡æ¯ç”¨äºç›‘æ§
        self.extra_info["est_vel"] = vel_mean
        self.extra_info["obs_predict"] = decode * (self.actor_obs_normalizer.std[:self.num_decode] + 1e-2) + \
                                                   self.actor_obs_normalizer.mean[:self.num_decode]
        
        return self.distribution.sample(), self.extra_info

    def act_inference(self, obs: TensorDict) -> tuple[torch.Tensor, dict]:
        """æ¨ç†æ—¶çš„ç¡®å®šæ€§åŠ¨ä½œ"""
        # 1. è·å–å¹¶å½’ä¸€åŒ–æœ¬ä½“è§‚æµ‹
        proprio_obs = self.get_actor_obs(obs)
        proprio_obs = self.actor_obs_normalizer(proprio_obs)
        
        # 2. ç¼–ç å™¨å‰å‘ä¼ æ’­
        code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar = \
            self.encoder_forward(proprio_obs, obs)
        
        # 3. æ¨ç†æ—¶ä½¿ç”¨å‡å€¼è€Œéé‡‡æ ·å€¼
        now_obs = proprio_obs[:, 0:self.obs_one_frame_len]  # å–å½“å‰è§‚æµ‹å€¼éƒ¨åˆ†
        observation = torch.cat((vel_mean.detach(), latent_mean.detach(), now_obs), dim=-1)
        
        # 4. Actorè¾“å‡ºç¡®å®šæ€§åŠ¨ä½œ
        mean = self.actor(observation)
        
        # 5. è®°å½•é¢å¤–ä¿¡æ¯
        self.extra_info["est_vel"] = vel_mean
        self.extra_info["obs_predict"] = decode * (self.actor_obs_normalizer.std[:self.num_decode] + 1e-2) + \
                                        self.actor_obs_normalizer.mean[:self.num_decode]
        
        return mean, self.extra_info

    def evaluate(self, obs: TensorDict, **kwargs: dict[str, Any]) -> torch.Tensor:
        """è¯„ä¼°çŠ¶æ€ä»·å€¼"""
        obs = self.get_critic_obs(obs)
        obs = self.critic_obs_normalizer(obs)
        return self.critic(obs)

    def get_actor_obs(self, obs: TensorDict) -> torch.Tensor:
        """è·å–actorçš„æœ¬ä½“è§‚æµ‹(æ’é™¤é«˜ç¨‹å›¾)"""
        obs_list = []
        for obs_group in self.obs_groups["policy"]:
            if obs_group != "height_scan_history":
                obs_list.append(obs[obs_group])
        return torch.cat(obs_list, dim=-1) if obs_list else torch.empty(obs[self.obs_groups["policy"][0]].shape[0], 0)

    def get_critic_obs(self, obs: TensorDict) -> torch.Tensor:
        """è·å–criticè§‚æµ‹(æ’é™¤é«˜ç¨‹å›¾)"""
        obs_list = []
        for obs_group in self.obs_groups["critic"]:
            if obs_group != "height_scan_history":
                obs_list.append(obs[obs_group])
        return torch.cat(obs_list, dim=-1) if obs_list else torch.empty(obs[self.obs_groups["critic"][0]].shape[0], 0)

    def get_actions_log_prob(self, actions: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡"""
        return self.distribution.log_prob(actions).sum(dim=-1)

    def update_normalization(self, obs: TensorDict) -> None:
        """æ›´æ–°è§‚æµ‹å½’ä¸€åŒ–ç»Ÿè®¡é‡"""
        if self.actor_obs_normalization:
            actor_obs = self.get_actor_obs(obs)
            self.actor_obs_normalizer.update(actor_obs)
        if self.critic_obs_normalization:
            critic_obs = self.get_critic_obs(obs)
            self.critic_obs_normalizer.update(critic_obs)

    def update_encoder(
        self,
        obs_batch: TensorDict,
        next_observations_batch: TensorDict,
        encoder_optimizer: torch.optim.Optimizer,
        max_grad_norm: float
    ) -> dict[str, float]:
        """æ›´æ–°VAEç¼–ç å™¨
        
        Args:
            obs_batch: å½“å‰è§‚æµ‹æ‰¹æ¬¡æ•°æ®
            next_observations_batch: ä¸‹ä¸€æ—¶åˆ»è§‚æµ‹æ‰¹æ¬¡æ•°æ®
            encoder_optimizer: ç¼–ç å™¨ä¼˜åŒ–å™¨
            max_grad_norm: æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°
            
        Returns:
            æŸå¤±å­—å…¸ï¼ŒåŒ…å«å„é¡¹æŸå¤±å€¼
        """
        # è·å–å¹¶å½’ä¸€åŒ–policyè§‚æµ‹
        policy_obs = self.get_actor_obs(obs_batch)
        policy_obs = self.actor_obs_normalizer(policy_obs)

        # å‰å‘ä¼ æ’­å¾—åˆ°ç¼–ç å™¨è¾“å‡º
        code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar = \
            self.encoder_forward(policy_obs, obs_batch)

        # è·å–å¹¶å½’ä¸€åŒ–criticè§‚æµ‹ï¼Œæå–çœŸå®é€Ÿåº¦
        critic_obs = self.get_critic_obs(obs_batch)
        critic_obs = self.critic_obs_normalizer(critic_obs)
        vel_target = critic_obs[:, 0:3]  # çœŸå®é€Ÿåº¦ä½œä¸ºç›®æ ‡

        # è·å–ä¸‹ä¸€æ—¶åˆ»è§‚æµ‹ï¼Œæå–ç›®æ ‡è§‚æµ‹
        next_observations = self.get_actor_obs(next_observations_batch)
        next_observations = self.actor_obs_normalizer(next_observations)
        obs_target = next_observations[:, 0:self.num_decode]  # å–æœ€æ–°è§‚æµ‹

        vel_target.requires_grad = False
        obs_target.requires_grad = False

        # æŸå¤±è®¡ç®—ï¼šé€Ÿåº¦é‡å»ºæŸå¤± + obsé‡å»ºæŸå¤± + KLæ•£åº¦æŸå¤±
        vel_MSE = nn.MSELoss()(vel_sample, vel_target)
        obs_MSE = nn.MSELoss()(decode, obs_target)
        dkl_loss = -0.5 * torch.mean(torch.sum(1 + latent_logvar - latent_mean.pow(2) - latent_logvar.exp(), dim=1))
        autoenc_loss = vel_MSE + obs_MSE + self.beta * dkl_loss

        # åå‘ä¼ æ’­
        encoder_optimizer.zero_grad()
        autoenc_loss.backward(retain_graph=True)

        # æ¢¯åº¦è£å‰ª
        encoder_params = [p for group in encoder_optimizer.param_groups for p in group['params']]
        nn.utils.clip_grad_norm_(encoder_params, max_grad_norm)

        # æ›´æ–°å‚æ•°
        encoder_optimizer.step()

        return {
            "vel_loss": vel_MSE.item(),
            "obs_loss": obs_MSE.item(),
            "dkl_loss": dkl_loss.item(),
            "total_loss": autoenc_loss.item(),
        }

    def load_state_dict(self, state_dict: dict, strict: bool = True) -> bool:
        """åŠ è½½æ¨¡å‹å‚æ•°"""
        super().load_state_dict(state_dict, strict=strict)
        return True

    def create_optimizers(self, learning_rate: float) -> dict[str, torch.optim.Optimizer]:
        """åˆ›å»ºä¼˜åŒ–å™¨
        
        Args:
            learning_rate: å­¦ä¹ ç‡
            
        Returns:
            ä¼˜åŒ–å™¨å­—å…¸ï¼ŒåŒ…å«ä¸»è¦çš„ä¼˜åŒ–å™¨å’Œç¼–ç å™¨ä¼˜åŒ–å™¨
        """
        import torch.optim as optim
        
        optimizer = optim.Adam([
            {'params': self.actor.parameters()},
            {'params': self.critic.parameters()},
            {'params': [self.std] if self.noise_std_type == "scalar" else [self.log_std]},
        ], lr=learning_rate)
        
        encoder_optimizer = optim.Adam([
            {'params': self.proprio_encoder.parameters()},
            {'params': self.elevation_net.parameters()},
            {'params': self.fusion_encoder.parameters()},
            {'params': self.encoder_latent_mean.parameters()},
            {'params': self.encoder_latent_logvar.parameters()},
            {'params': self.encoder_vel_mean.parameters()},
            {'params': self.encoder_vel_logvar.parameters()},
            {'params': self.decoder.parameters()},
        ], lr=learning_rate)
        
        return {
            "optimizer": optimizer,
            "encoder_optimizer": encoder_optimizer
        }

    def export_to_onnx(self, path: str, filename: str = "ElevationNet_mode3_policy.onnx", normalizer: torch.nn.Module | None = None, verbose: bool = False) -> None:
        """å°†ElevationNet Mode3ç­–ç•¥å¯¼å‡ºä¸ºONNXæ ¼å¼
        
        Args:
            path: ä¿å­˜ç›®å½•çš„è·¯å¾„
            filename: å¯¼å‡ºçš„ONNXæ–‡ä»¶åï¼Œé»˜è®¤ä¸º"ElevationNet_mode3_policy.onnx"
            normalizer: å½’ä¸€åŒ–æ¨¡å—ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨Identity
            verbose: æ˜¯å¦æ‰“å°æ¨¡å‹æ‘˜è¦ï¼Œé»˜è®¤ä¸ºFalse
        """
        import copy
        import os
        
        if not os.path.exists(path):
            os.makedirs(path, exist_ok=True)
            
        # åˆ›å»ºElevationNet Mode3ä¸“ç”¨çš„å¯¼å‡ºå™¨
        exporter = _ElevationNetMode3OnnxPolicyExporter(self, normalizer, verbose)
        exporter.export(path, filename)


class _ElevationNetMode3OnnxPolicyExporter(torch.nn.Module):
    """ElevationNet Mode3ç­–ç•¥çš„ONNXå¯¼å‡ºå™¨"""

    def __init__(self, policy: ActorCriticElevationNetMode3, normalizer=None, verbose=False):
        super().__init__()
        self.verbose = verbose
        # å¤åˆ¶ç­–ç•¥å‚æ•°
        if hasattr(policy, "proprio_encoder"):
            self.proprio_encoder = copy.deepcopy(policy.proprio_encoder)
        if hasattr(policy, "elevation_net"):
            self.elevation_net = copy.deepcopy(policy.elevation_net)
        if hasattr(policy, "fusion_encoder"):
            self.fusion_encoder = copy.deepcopy(policy.fusion_encoder)
        if hasattr(policy, "encoder_latent_mean"):
            self.encoder_latent_mean = copy.deepcopy(policy.encoder_latent_mean)
        if hasattr(policy, "encoder_latent_logvar"):
            self.encoder_latent_logvar = copy.deepcopy(policy.encoder_latent_logvar)
        if hasattr(policy, "encoder_vel_mean"):
            self.encoder_vel_mean = copy.deepcopy(policy.encoder_vel_mean)
        if hasattr(policy, "encoder_vel_logvar"):
            self.encoder_vel_logvar = copy.deepcopy(policy.encoder_vel_logvar)
        if hasattr(policy, "actor"):
            self.actor = copy.deepcopy(policy.actor)
        
        self.obs_one_frame_len = policy.obs_one_frame_len

        # å¤åˆ¶å½’ä¸€åŒ–å™¨
        if normalizer:
            self.normalizer = copy.deepcopy(normalizer)
        else:
            self.normalizer = torch.nn.Identity()

    def forward(self, x):
        # å‡è®¾è¾“å…¥æ˜¯ [æœ¬ä½“è§‚æµ‹ + é«˜ç¨‹å›¾åºåˆ—å±•å¹³]
        obs_len = self.normalizer.in_features
        proprio_obs = x[:, 0:obs_len]
        height_map_sequence = x[:, obs_len:]
        
        # å½’ä¸€åŒ–æœ¬ä½“è§‚æµ‹
        normalized_obs = self.normalizer(proprio_obs)
        
        # æå–ç‰¹å¾
        proprio_features = self.proprio_encoder(normalized_obs)
        vision_features = self.elevation_net(height_map_sequence)
        
        # èåˆç‰¹å¾
        fused_features = torch.cat([proprio_features, vision_features], dim=-1)
        x = self.fusion_encoder(fused_features)
        
        # VAEç¼–ç ï¼šä½¿ç”¨å‡å€¼ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        latent_mean = self.encoder_latent_mean(x)
        vel_mean = self.encoder_vel_mean(x)
        
        # æ‹¼æ¥éšå‘é‡
        code = torch.cat((vel_mean, latent_mean), dim=-1)
        
        # ä¸å½“å‰æœ¬ä½“è§‚æµ‹æ‹¼æ¥
        now_obs = normalized_obs[:, 0:self.obs_one_frame_len]
        observation = torch.cat((code.detach(), now_obs), dim=-1)
        
        # è¾“å‡ºåŠ¨ä½œ
        actions_mean = self.actor(observation)
        return actions_mean, vel_mean

    def export(self, path, filename):
        self.to("cpu")
        self.eval()
        opset_version = 18
        # åˆ›å»ºè¾“å…¥ç¤ºä¾‹
        total_dim = self.normalizer.in_features + self.elevation_net[0].in_features
        obs = torch.zeros(1, total_dim)
        torch.onnx.export(
            self,
            obs,
            os.path.join(path, filename),
            export_params=True,
            opset_version=opset_version,
            verbose=self.verbose,
            input_names=["obs"],
            output_names=["actions", "est_vel"],
            dynamic_axes={},
        )
