# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
ActorCriticElevationNetMode5: VAEç¼–ç å™¨æ¶æ„ï¼Œä½¿ç”¨3D CNNå¤„ç†é«˜ç¨‹å›¾åºåˆ—

ç½‘ç»œç»“æ„:
    æœ¬ä½“ -> æœ¬ä½“ç¼–ç å™¨MLP -> ç‰¹å¾1
    é«˜ç¨‹å›¾åºåˆ— -> 3D CNNç¼–ç å™¨(æ—¶ç©ºå·ç§¯) -> ç‰¹å¾2
    [ç‰¹å¾1 + ç‰¹å¾2] -> èåˆMLP -> ç¼–ç ç‰¹å¾
    ç¼–ç ç‰¹å¾ -> Encoder -> éšå‘é‡(v+z)
    éšå‘é‡ -> Decoder -> é‡å»ºè§‚æµ‹
    [éšå‘é‡ + æœ¬ä½“] -> Actor MLP -> åŠ¨ä½œ
"""

from __future__ import annotations

import torch
import torch.nn as nn
from tensordict import TensorDict
from torch.distributions import Normal
from typing import Any, NoReturn

from rsl_rl.networks import MLP, EmpiricalNormalization
import copy
import os

class Conv3DEncoder(nn.Module):
    """3D CNNç¼–ç å™¨ï¼Œç”¨äºå¤„ç†é«˜ç¨‹å›¾åºåˆ—ï¼ˆæ—¶ç©ºå·ç§¯ï¼‰"""
    
    def __init__(
        self,
        input_channels: int,
        sequence_length: int,
        spatial_size: tuple[int, int],
        output_dim: int,
        hidden_dims: list[int] = [16, 32, 64],
        kernel_sizes: list[list[int]] = [[3, 3, 3], [3, 3, 3], [3, 3, 3]],
        activation: str = "elu"
    ) -> None:
        super().__init__()
        
        self.sequence_length = sequence_length
        self.spatial_size = spatial_size
        self.output_dim = output_dim
        
        # éªŒè¯kernel_sizesæ ¼å¼ï¼šå¿…é¡»æ˜¯äºŒé‡æ•°ç»„ï¼Œæ¯ä¸ªå­æ•°ç»„è¡¨ç¤º[æ—¶é—´æ·±åº¦, é«˜åº¦, å®½åº¦]
        for i, kernel_size in enumerate(kernel_sizes):
            if len(kernel_size) != 3:
                raise ValueError(f"3D CNN kernel_size should be [temporal, height, width], got {kernel_size}")
        
        # æ„å»º3Då·ç§¯å±‚ï¼ŒåŒæ—¶å¤„ç†æ—¶é—´å’Œç©ºé—´ç»´åº¦
        layers = []
        in_channels = input_channels
        
        for i, (hidden_dim, kernel_size) in enumerate(zip(hidden_dims, kernel_sizes)):
            # 3Då·ç§¯æ ¸: (æ—¶é—´æ·±åº¦, é«˜åº¦, å®½åº¦)
            temporal_kernel, spatial_kernel_h, spatial_kernel_w = kernel_size
            padding = (temporal_kernel // 2, spatial_kernel_h // 2, spatial_kernel_w // 2)  # ä¿æŒæ—¶ç©ºå°ºå¯¸
            
            layers.extend([
                nn.Conv3d(
                    in_channels, 
                    hidden_dim,
                    kernel_size=kernel_size,
                    padding=padding
                ),
                nn.BatchNorm3d(hidden_dim),
                nn.ELU() if activation == "elu" else nn.ReLU()
            ])
            in_channels = hidden_dim
        
        self.conv_layers = nn.Sequential(*layers)
        
        # è®¡ç®—å·ç§¯åçš„ç‰¹å¾å›¾å°ºå¯¸
        # ç»è¿‡3Då·ç§¯åï¼Œæ—¶ç©ºå°ºå¯¸ä¿æŒä¸å˜
        seq_len, height, width = sequence_length, spatial_size[0], spatial_size[1]
        self.flattened_size = seq_len * height * width * in_channels
        
        # å…¨è¿æ¥å±‚å°†ç‰¹å¾æ˜ å°„åˆ°è¾“å‡ºç»´åº¦
        self.fc = nn.Linear(self.flattened_size, output_dim)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, sequence_length, height*width] æˆ– [batch_size, sequence_length, height, width]
        Returns:
            ç‰¹å¾å‘é‡ [batch_size, output_dim]
        """
        height, width = self.spatial_size
        
        # å¤„ç†ä¸¤ç§è¾“å…¥æ ¼å¼
        if x.dim() == 3:
            # è¾“å…¥æ ¼å¼: [batch_size, sequence_length, height*width]
            batch_size, seq_len, flat_dim = x.shape
            
            # éªŒè¯å±•å¹³ç»´åº¦æ˜¯å¦åŒ¹é…
            expected_flat_dim = height * width
            if flat_dim != expected_flat_dim:
                raise ValueError(f"Expected flattened dimension {expected_flat_dim} (height={height} * width={width}), got {flat_dim}")
            
            # Reshapeæˆ [batch_size, 1, sequence_length, height, width]
            x = x.view(batch_size, seq_len, height, width)
            x = x.unsqueeze(1)  # [batch_size, 1, sequence_length, height, width]
            
        elif x.dim() == 4:
            # è¾“å…¥æ ¼å¼: [batch_size, sequence_length, height, width]
            batch_size, seq_len, h, w = x.shape
            
            # éªŒè¯ç©ºé—´ç»´åº¦æ˜¯å¦åŒ¹é…
            if h != height or w != width:
                raise ValueError(f"Expected spatial size ({height}, {width}), got ({h}, {w})")
            
            # æ·»åŠ é€šé“ç»´åº¦ä¸º1ï¼Œç”¨äº3Då·ç§¯
            x = x.unsqueeze(1)  # [batch_size, 1, sequence_length, height, width]
            
        else:
            raise ValueError(f"Expected 3D or 4D input, got {x.dim()}D input with shape {x.shape}")
        
        # 3Då·ç§¯å‰å‘ä¼ æ’­
        conv_features = self.conv_layers(x)  # [batch_size, hidden_dim, sequence_length, height, width]
        
        # å±•å¹³ç‰¹å¾å›¾
        conv_features_flat = conv_features.view(conv_features.size(0), -1)
        
        # å…¨è¿æ¥å±‚
        output = self.fc(conv_features_flat)
        
        return output


class ActorCriticElevationNetMode5(nn.Module):
    """Mode5: ä½¿ç”¨3D CNNå¤„ç†é«˜ç¨‹å›¾åºåˆ—ï¼ˆæ—¶ç©ºå·ç§¯ï¼‰+ VAEæ¶æ„
    
    ç½‘ç»œç»„æˆ:
    1. æœ¬ä½“ç¼–ç å™¨MLP - æå–æœ¬ä½“ç‰¹å¾
    2. é«˜ç¨‹å›¾3D CNNç¼–ç å™¨ - æ—¶ç©ºå·ç§¯æå–æ—¶ç©ºç‰¹å¾
    3. èåˆMLP - èåˆç‰¹å¾
    4. VAE Encoder - è¾“å‡ºéšå‘é‡(é€Ÿåº¦v + éšçŠ¶æ€z)
    5. VAE Decoder - é‡å»ºè§‚æµ‹
    6. Actor MLP - ä»éšå‘é‡+æœ¬ä½“è§‚æµ‹è¾“å‡ºåŠ¨ä½œ
    7. Critic MLP - ä»·å€¼è¯„ä¼°
    """
    
    is_recurrent: bool = False

    def __init__(
        self,
        obs: TensorDict,
        obs_groups: dict[str, list[str]],
        num_actions: int,
        env_cfg=None,
        actor_obs_normalization: bool = False,
        critic_obs_normalization: bool = False,
        actor_hidden_dims: tuple[int] | list[int] = [128, 64],
        critic_hidden_dims: tuple[int] | list[int] = [256, 128, 64],
        activation: str = "elu",
        init_noise_std: float = 1.0,
        noise_std_type: str = "scalar",
        # é«˜ç¨‹å›¾ç¼–ç å™¨é…ç½®
        vision_feature_dim: int = 32,
        history_frames: int = 5,
        vision_spatial_size: tuple[int, int] = (25, 17),
        elevation_encoder_hidden_dims: list[int] | None = None,
        # 3D CNNé…ç½®
        conv3d_hidden_dims: list[int] = [16, 32, 64],
        conv3d_kernel_sizes: list[list[int]] = [[3, 3, 3], [3, 3, 3], [3, 3, 3]],
        # æœ¬ä½“ç¼–ç å™¨é…ç½®
        proprio_feature_dim: int = 64,
        proprio_encoder_hidden_dims: list[int] | None = None,
        # èåˆç½‘ç»œé…ç½®
        fusion_actor_hidden_dims: list[int] | None = None,
        # VAEç¼–ç å™¨-è§£ç å™¨é…ç½®
        encoder_hidden_dims: tuple[int] | list[int] = [1024, 512, 256],
        decoder_hidden_dims: tuple[int] | list[int] = [256, 512, 1024],
        num_latent: int = 19,
        num_decode: int = 30,
        VAE_beta: float = 1.0,
        **kwargs: dict[str, Any],
    ) -> None:
        super().__init__()

        # é…ç½®
        self.cfg = kwargs
        self.extra_info = dict()
        self.obs_groups = obs_groups
        self.vision_spatial_size = vision_spatial_size
        self.noise_std_type = noise_std_type
        self.beta = VAE_beta
        self.num_decode = num_decode
        num_actor_obs = 0
        
        # è®¡ç®—è§‚æµ‹ç»´åº¦
        num_actor_obs = sum(obs[g].shape[-1] for g in obs_groups["policy"] if g != "height_scan_history")
        num_critic_obs = sum(obs[g].shape[-1] for g in obs_groups["critic"] if g != "height_scan_history")
        # ä¿®å¤è§‚æµ‹ç»´åº¦è®¡ç®—é”™è¯¯ï¼šobs_one_frame_lenåº”è¯¥æ˜¯å•å¸§è§‚æµ‹ç»´åº¦ï¼Œä¸æ˜¯æ€»è§‚æµ‹é™¤ä»¥å†å²å¸§æ•°
        self.obs_one_frame_len: int = 102  # G1å•å¸§æœ¬ä½“è§‚æµ‹ç»´åº¦å›ºå®šä¸º102
        
        ########################################## Actor ##############################################
        print("\n" + "=" * 80)
        print("ğŸŒŸ ç½‘ç»œæ¶æ„: ElevationNet Mode5 (3D CNN + VAE)")
        print("=" * 80)
        print("âœ“ Mode5: æœ¬ä½“ç¼–ç å™¨ + é«˜ç¨‹å›¾3D CNNç¼–ç å™¨(æ—¶ç©ºå·ç§¯) + èåˆç½‘ç»œ + VAE -> åŠ¨ä½œ")
        
        # 1. æœ¬ä½“ç¼–ç å™¨MLP
        if proprio_encoder_hidden_dims is None:
            proprio_encoder_hidden_dims = actor_hidden_dims
        self.proprio_encoder = MLP(num_actor_obs, proprio_feature_dim, proprio_encoder_hidden_dims, activation)
        print(f"  1. æœ¬ä½“ç¼–ç å™¨: {num_actor_obs} -> {proprio_encoder_hidden_dims} -> {proprio_feature_dim}")
        
        # 2. é«˜ç¨‹å›¾3D CNNç¼–ç å™¨ï¼ˆæ—¶ç©ºå·ç§¯ï¼‰
        height, width = vision_spatial_size
        self.elevation_net = Conv3DEncoder(
            input_channels=1,
            sequence_length=history_frames,
            spatial_size=vision_spatial_size,
            output_dim=vision_feature_dim,
            hidden_dims=conv3d_hidden_dims,
            kernel_sizes=conv3d_kernel_sizes,
            activation=activation
        )
        print(f"  2. é«˜ç¨‹å›¾3D CNN: [{history_frames}, {height}, {width}] -> {conv3d_hidden_dims} -> {vision_feature_dim}")
        
        # 3. èåˆMLP
        fusion_output_dim = encoder_hidden_dims[-1]
        if fusion_actor_hidden_dims is None:
            fusion_actor_hidden_dims = actor_hidden_dims if actor_hidden_dims else [256, 128]
            print(f"     â„¹ï¸  fusion_actor_hidden_dimsæœªè®¾ç½®ï¼Œä½¿ç”¨actor_hidden_dims={fusion_actor_hidden_dims}")
        fusion_input_dim = proprio_feature_dim + vision_feature_dim
        self.fusion_encoder = MLP(fusion_input_dim, fusion_output_dim, fusion_actor_hidden_dims, "elu")
        print(f"  3. èåˆMLP: {fusion_input_dim} (æœ¬ä½“{proprio_feature_dim} + è§†è§‰{vision_feature_dim}) -> {fusion_actor_hidden_dims} -> {fusion_output_dim}")
        
        # 4. VAE Encoder: è¾“å‡ºå‡å€¼å’Œæ–¹å·®
        self.encoder_latent_mean = nn.Linear(fusion_output_dim, num_latent - 3)
        self.encoder_latent_logvar = nn.Linear(fusion_output_dim, num_latent - 3)
        self.encoder_vel_mean = nn.Linear(fusion_output_dim, 3)
        self.encoder_vel_logvar = nn.Linear(fusion_output_dim, 3)
        print(f"  4. VAE Encoder: {fusion_output_dim} -> éšå‘é‡{num_latent} (é€Ÿåº¦3 + éšçŠ¶æ€{num_latent-3})")
        
        # 5. VAE Decoder: ä»éšå‘é‡é‡å»ºè§‚æµ‹
        self.decoder = MLP(num_latent, num_decode, decoder_hidden_dims, activation)
        print(f"  5. VAE Decoder: {num_latent} -> {decoder_hidden_dims} -> {num_decode}")
        
        # 6. Actor: ä»éšå‘é‡+å½“å‰æœ¬ä½“è§‚æµ‹è¾“å‡ºåŠ¨ä½œ
        actor_input_dim = num_latent + self.obs_one_frame_len
        self.actor = MLP(actor_input_dim, num_actions, actor_hidden_dims, activation)
        print(f"  6. Actor MLP: {actor_input_dim} (éšå‘é‡{num_latent} + æœ¬ä½“{self.obs_one_frame_len}) -> {actor_hidden_dims} -> {num_actions}")

        # Actor observation normalization
        self.actor_obs_normalization = actor_obs_normalization
        if actor_obs_normalization:
            self.actor_obs_normalizer = EmpiricalNormalization(num_actor_obs)
        else:
            self.actor_obs_normalizer = torch.nn.Identity()

        ########################################## Critic ##############################################
        self.critic = MLP(num_critic_obs, 1, critic_hidden_dims, activation)
        print(f"  7. Critic MLP: {num_critic_obs} -> {critic_hidden_dims} -> 1")
        print(f"\n  VAE Beta: {VAE_beta}")
        print("=" * 80 + "\n")

        # Critic observation normalization
        self.critic_obs_normalization = critic_obs_normalization
        if critic_obs_normalization:
            self.critic_obs_normalizer = EmpiricalNormalization(num_critic_obs)
        else:
            self.critic_obs_normalizer = torch.nn.Identity()

        ########################################## Action Noise ##############################################
        if self.noise_std_type == "scalar":
            self.std = nn.Parameter(init_noise_std * torch.ones(num_actions))
        elif self.noise_std_type == "log":
            self.log_std = nn.Parameter(torch.log(init_noise_std * torch.ones(num_actions)))
        else:
            raise ValueError(f"Unknown standard deviation type: {self.noise_std_type}")

        # Action distribution
        self.distribution = None
        Normal.set_default_validate_args(False)

    def reset(self, dones: torch.Tensor | None = None) -> None:
        pass

    def forward(self) -> NoReturn:
        raise NotImplementedError

    @property
    def action_mean(self) -> torch.Tensor:
        return self.distribution.mean

    @property
    def action_std(self) -> torch.Tensor:
        return self.distribution.stddev

    @property
    def entropy(self) -> torch.Tensor:
        return self.distribution.entropy().sum(dim=-1)

    def _extract_height_map_sequence(self, obs: TensorDict) -> torch.Tensor:
        """æå–é«˜ç¨‹å›¾åºåˆ—ç”¨äº3D CNNå¤„ç†ï¼ˆæ—¶ç©ºå·ç§¯ï¼‰"""
        depth_obs = obs["height_scan_history"]
        while isinstance(depth_obs, TensorDict):
            keys = list(depth_obs.keys())
            depth_obs = depth_obs[keys[0]]
        
        # depth_obs å½¢çŠ¶: [batch_size, history_frames, height, width]
        # è¿™ä¸ªå½¢çŠ¶é€‚åˆConv3DEncoderå¤„ç†ï¼ˆå°†è¿›è¡Œæ—¶ç©ºå·ç§¯ï¼‰
        return depth_obs

    def reparameterise(self, mean: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        """é‡å‚æ•°åŒ–æŠ€å·§"""
        std = torch.exp(logvar * 0.5)
        code_temp = torch.randn_like(std)
        code = mean + std * code_temp
        return code
    
    def encoder_forward(self, proprio_obs: torch.Tensor, obs: TensorDict):
        """ç¼–ç å™¨å‰å‘ä¼ æ’­"""
        # 1. æå–æœ¬ä½“ç‰¹å¾
        proprio_features = self.proprio_encoder(proprio_obs)
        
        # 2. æå–é«˜ç¨‹å›¾ç‰¹å¾ï¼ˆä½¿ç”¨3D CNNï¼Œæ—¶ç©ºå·ç§¯ï¼‰
        height_map_sequence = self._extract_height_map_sequence(obs) # TODO: é«˜ç¨‹å›¾æ˜¯å¦éœ€è¦å½’ä¸€åŒ–
        vision_features = self.elevation_net(height_map_sequence)
        
        # 3. èåˆç‰¹å¾
        fused_features = torch.cat([proprio_features, vision_features], dim=-1)
        x = self.fusion_encoder(fused_features)
        
        # 4. VAEç¼–ç : è¾“å‡ºå‡å€¼å’Œæ–¹å·®
        latent_mean = self.encoder_latent_mean(x)
        latent_logvar = self.encoder_latent_logvar(x)
        vel_mean = self.encoder_vel_mean(x)
        vel_logvar = self.encoder_vel_logvar(x)
        
        # é™åˆ¶æ–¹å·®èŒƒå›´
        latent_logvar = torch.clip(latent_logvar, min=-10, max=10)
        vel_logvar = torch.clip(vel_logvar, min=-10, max=10)
        
        # 5. é‡‡æ ·éšå‘é‡
        latent_sample = self.reparameterise(latent_mean, latent_logvar)
        vel_sample = self.reparameterise(vel_mean, vel_logvar)
        
        # 6. æ‹¼æ¥æˆå®Œæ•´éšå‘é‡
        code = torch.cat((vel_sample, latent_sample), dim=-1)
        
        # 7. è§£ç 
        decode = self.decoder(code)
        
        return code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar

    def _update_distribution(self, mean: torch.Tensor) -> None:
        """æ›´æ–°åŠ¨ä½œåˆ†å¸ƒ"""
        if self.noise_std_type == "scalar":
            std = self.std.expand_as(mean)
        elif self.noise_std_type == "log":
            std = torch.exp(self.log_std).expand_as(mean)
        
        self.distribution = Normal(mean, std)

    def act(self, obs: TensorDict, **kwargs: dict[str, Any]) -> tuple[torch.Tensor, dict]:
        """è®­ç»ƒæ—¶çš„åŠ¨ä½œé‡‡æ ·"""
        # 1. è·å–å¹¶å½’ä¸€åŒ–æœ¬ä½“è§‚æµ‹
        proprio_obs = self.get_actor_obs(obs)
        proprio_obs = self.actor_obs_normalizer(proprio_obs)
        
        # 2. ç¼–ç å™¨å‰å‘ä¼ æ’­å¾—åˆ°éšå‘é‡
        code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar = \
            self.encoder_forward(proprio_obs, obs)
        
        # 3. å°†éšå‘é‡ä¸å½“å‰æœ¬ä½“è§‚æµ‹æ‹¼æ¥
        now_obs = proprio_obs[:, 0:self.obs_one_frame_len]  # å–å½“å‰è§‚æµ‹å€¼éƒ¨åˆ†
        observation = torch.cat((code.detach(), now_obs), dim=-1)
        
        # 4. Actorè¾“å‡ºåŠ¨ä½œ
        mean = self.actor(observation)
        self._update_distribution(mean)
        
        # 5. è®°å½•é¢å¤–ä¿¡æ¯ç”¨äºç›‘æ§
        self.extra_info["est_vel"] = vel_mean
        if self.actor_obs_normalization:
            self.extra_info["obs_predict"] = decode * (self.actor_obs_normalizer.std[:self.num_decode] + 1e-2) + \
                                                       self.actor_obs_normalizer.mean[:self.num_decode]
        else:
            self.extra_info["obs_predict"] = decode
        
        return self.distribution.sample(), self.extra_info

    def act_inference(self, obs: TensorDict) -> tuple[torch.Tensor, dict]:
        """æ¨ç†æ—¶çš„ç¡®å®šæ€§åŠ¨ä½œ"""
        # 1. è·å–å¹¶å½’ä¸€åŒ–æœ¬ä½“è§‚æµ‹
        proprio_obs = self.get_actor_obs(obs)
        proprio_obs = self.actor_obs_normalizer(proprio_obs)
        
        # 2. ç¼–ç å™¨å‰å‘ä¼ æ’­
        code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar = \
            self.encoder_forward(proprio_obs, obs)
        
        # 3. æ¨ç†æ—¶ä½¿ç”¨å‡å€¼è€Œéé‡‡æ ·å€¼
        now_obs = proprio_obs[:, 0:self.obs_one_frame_len]  # å–å½“å‰è§‚æµ‹å€¼éƒ¨åˆ†
        observation = torch.cat((vel_mean.detach(), latent_mean.detach(), now_obs), dim=-1)
        
        # 4. Actorè¾“å‡ºç¡®å®šæ€§åŠ¨ä½œ
        mean = self.actor(observation)
        
        # 5. è®°å½•é¢å¤–ä¿¡æ¯
        self.extra_info["est_vel"] = vel_mean
        if self.actor_obs_normalization:
            self.extra_info["obs_predict"] = decode * (self.actor_obs_normalizer.std[:self.num_decode] + 1e-2) + \
                                            self.actor_obs_normalizer.mean[:self.num_decode]
        else:
            self.extra_info["obs_predict"] = decode
        
        return mean, self.extra_info

    def evaluate(self, obs: TensorDict, **kwargs: dict[str, Any]) -> torch.Tensor:
        """è¯„ä¼°çŠ¶æ€ä»·å€¼"""
        obs = self.get_critic_obs(obs)
        obs = self.critic_obs_normalizer(obs)
        return self.critic(obs)

    def get_actor_obs(self, obs: TensorDict) -> torch.Tensor:
        """è·å–actorçš„æœ¬ä½“è§‚æµ‹(æ’é™¤é«˜ç¨‹å›¾)"""
        obs_list = []
        for obs_group in self.obs_groups["policy"]:
            if obs_group != "height_scan_history":
                obs_list.append(obs[obs_group])
        return torch.cat(obs_list, dim=-1) if obs_list else torch.empty(obs[self.obs_groups["policy"][0]].shape[0], 0)

    def get_critic_obs(self, obs: TensorDict) -> torch.Tensor:
        """è·å–criticè§‚æµ‹(æ’é™¤é«˜ç¨‹å›¾)"""
        obs_list = []
        for obs_group in self.obs_groups["critic"]:
            if obs_group != "height_scan_history":
                obs_list.append(obs[obs_group])
        return torch.cat(obs_list, dim=-1) if obs_list else torch.empty(obs[self.obs_groups["critic"][0]].shape[0], 0)

    def get_actions_log_prob(self, actions: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡"""
        return self.distribution.log_prob(actions).sum(dim=-1)

    def update_normalization(self, obs: TensorDict) -> None:
        """æ›´æ–°è§‚æµ‹å½’ä¸€åŒ–ç»Ÿè®¡é‡"""
        if self.actor_obs_normalization:
            actor_obs = self.get_actor_obs(obs)
            self.actor_obs_normalizer.update(actor_obs)
        if self.critic_obs_normalization:
            critic_obs = self.get_critic_obs(obs)
            self.critic_obs_normalizer.update(critic_obs)

    def update_encoder(
        self,
        obs_batch: TensorDict,
        next_observations_batch: TensorDict,
        encoder_optimizer: torch.optim.Optimizer,
        max_grad_norm: float
    ) -> dict[str, float]:
        """æ›´æ–°VAEç¼–ç å™¨
        
        Args:
            obs_batch: å½“å‰è§‚æµ‹æ‰¹æ¬¡æ•°æ®
            next_observations_batch: ä¸‹ä¸€æ—¶åˆ»è§‚æµ‹æ‰¹æ¬¡æ•°æ®
            encoder_optimizer: ç¼–ç å™¨ä¼˜åŒ–å™¨
            max_grad_norm: æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°
            
        Returns:
            æŸå¤±å­—å…¸ï¼ŒåŒ…å«å„é¡¹æŸå¤±å€¼
        """
        # è·å–å¹¶å½’ä¸€åŒ–policyè§‚æµ‹
        policy_obs = self.get_actor_obs(obs_batch)
        policy_obs = self.actor_obs_normalizer(policy_obs)

        # å‰å‘ä¼ æ’­å¾—åˆ°ç¼–ç å™¨è¾“å‡º
        code, vel_sample, latent_sample, decode, vel_mean, vel_logvar, latent_mean, latent_logvar = \
            self.encoder_forward(policy_obs, obs_batch)

        # è·å–å¹¶å½’ä¸€åŒ–criticè§‚æµ‹ï¼Œæå–çœŸå®é€Ÿåº¦
        critic_obs = self.get_critic_obs(obs_batch)
        critic_obs = self.critic_obs_normalizer(critic_obs)
        vel_target = critic_obs[:, 0:3]  # çœŸå®é€Ÿåº¦ä½œä¸ºç›®æ ‡

        # è·å–ä¸‹ä¸€æ—¶åˆ»è§‚æµ‹ï¼Œæå–ç›®æ ‡è§‚æµ‹
        next_observations = self.get_actor_obs(next_observations_batch)
        next_observations = self.actor_obs_normalizer(next_observations)
        obs_target = next_observations[:, 0:self.num_decode]  # å–æœ€æ–°è§‚æµ‹

        vel_target.requires_grad = False
        obs_target.requires_grad = False

        # æŸå¤±è®¡ç®—ï¼šé€Ÿåº¦é‡å»ºæŸå¤± + obsé‡å»ºæŸå¤± + KLæ•£åº¦æŸå¤±
        vel_MSE = nn.MSELoss()(vel_sample, vel_target) * 100.0
        # ç¡®ä¿decodeå’Œobs_targetç»´åº¦åŒ¹é…
        decode_target = decode[:, :obs_target.shape[1]]  # æˆªå–åŒ¹é…çš„ç»´åº¦
        obs_MSE = nn.MSELoss()(decode_target, obs_target)
        dkl_loss = -0.5 * torch.mean(torch.sum(1 + latent_logvar - latent_mean.pow(2) - latent_logvar.exp(), dim=1))
        autoenc_loss = vel_MSE + obs_MSE + self.beta * dkl_loss

        # åå‘ä¼ æ’­
        encoder_optimizer.zero_grad()
        autoenc_loss.backward(retain_graph=True)

        # æ¢¯åº¦è£å‰ª
        encoder_params = [p for group in encoder_optimizer.param_groups for p in group['params']]
        nn.utils.clip_grad_norm_(encoder_params, max_grad_norm)

        # æ›´æ–°å‚æ•°
        encoder_optimizer.step()

        return {
            "vel_loss": vel_MSE.item(),
            "obs_loss": obs_MSE.item(),
            "dkl_loss": dkl_loss.item(),
            "total_loss": autoenc_loss.item(),
        }

    def load_state_dict(self, state_dict: dict, strict: bool = True) -> bool:
        """åŠ è½½æ¨¡å‹å‚æ•°"""
        super().load_state_dict(state_dict, strict=strict)
        return True

    def create_optimizers(self, learning_rate: float) -> dict[str, torch.optim.Optimizer]:
        """åˆ›å»ºä¼˜åŒ–å™¨
        
        Args:
            learning_rate: å­¦ä¹ ç‡
            
        Returns:
            ä¼˜åŒ–å™¨å­—å…¸ï¼ŒåŒ…å«ä¸»è¦çš„ä¼˜åŒ–å™¨å’Œç¼–ç å™¨ä¼˜åŒ–å™¨
        """
        import torch.optim as optim
        
        optimizer = optim.Adam([
            {'params': self.actor.parameters()},
            {'params': self.critic.parameters()},
            {'params': [self.std] if self.noise_std_type == "scalar" else [self.log_std]},
        ], lr=learning_rate)
        
        encoder_optimizer = optim.Adam([
            {'params': self.proprio_encoder.parameters()},
            {'params': self.elevation_net.parameters()},
            {'params': self.fusion_encoder.parameters()},
            {'params': self.encoder_latent_mean.parameters()},
            {'params': self.encoder_latent_logvar.parameters()},
            {'params': self.encoder_vel_mean.parameters()},
            {'params': self.encoder_vel_logvar.parameters()},
            {'params': self.decoder.parameters()},
        ], lr=learning_rate)
        
        return {
            "optimizer": optimizer,
            "encoder_optimizer": encoder_optimizer
        }

    def export_to_onnx(self, path: str, filename: str = "ElevationNet_mode5_policy.onnx", normalizer: torch.nn.Module | None = None, verbose: bool = False) -> None:
        """å°†ElevationNet Mode5ç­–ç•¥å¯¼å‡ºä¸ºONNXæ ¼å¼
        
        Args:
            path: ä¿å­˜ç›®å½•çš„è·¯å¾„
            filename: å¯¼å‡ºçš„ONNXæ–‡ä»¶åï¼Œé»˜è®¤ä¸º"ElevationNet_mode5_policy.onnx"
            normalizer: å½’ä¸€åŒ–æ¨¡å—ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨Identity
            verbose: æ˜¯å¦æ‰“å°æ¨¡å‹æ‘˜è¦ï¼Œé»˜è®¤ä¸ºFalse
        """
        import copy
        import os
        
        if not os.path.exists(path):
            os.makedirs(path, exist_ok=True)
            
        # åˆ›å»ºElevationNet Mode5ä¸“ç”¨çš„å¯¼å‡ºå™¨
        exporter = _ElevationNetMode5OnnxPolicyExporter(self, normalizer, verbose)
        exporter.export(path, filename)


class _ElevationNetMode5OnnxPolicyExporter(torch.nn.Module):
    """ElevationNet Mode5ç­–ç•¥çš„ONNXå¯¼å‡ºå™¨"""

    def __init__(self, policy: ActorCriticElevationNetMode5, normalizer=None, verbose=False):
        super().__init__()
        self.verbose = verbose
        # å¤åˆ¶ç­–ç•¥å‚æ•°
        if hasattr(policy, "proprio_encoder"):
            self.proprio_encoder = copy.deepcopy(policy.proprio_encoder)
        if hasattr(policy, "elevation_net"):
            self.elevation_net = copy.deepcopy(policy.elevation_net)
        if hasattr(policy, "fusion_encoder"):
            self.fusion_encoder = copy.deepcopy(policy.fusion_encoder)
        if hasattr(policy, "encoder_latent_mean"):
            self.encoder_latent_mean = copy.deepcopy(policy.encoder_latent_mean)
        if hasattr(policy, "encoder_latent_logvar"):
            self.encoder_latent_logvar = copy.deepcopy(policy.encoder_latent_logvar)
        if hasattr(policy, "encoder_vel_mean"):
            self.encoder_vel_mean = copy.deepcopy(policy.encoder_vel_mean)
        if hasattr(policy, "encoder_vel_logvar"):
            self.encoder_vel_logvar = copy.deepcopy(policy.encoder_vel_logvar)
        if hasattr(policy, "actor"):
            self.actor = copy.deepcopy(policy.actor)
        
        self.obs_one_frame_len = policy.obs_one_frame_len
        self.vision_spatial_size = policy.vision_spatial_size

        # å¤åˆ¶å½’ä¸€åŒ–å™¨
        if normalizer:
            self.normalizer = copy.deepcopy(normalizer)
        else:
            self.normalizer = torch.nn.Identity()

    def forward(self, x):
        # è¾“å…¥éœ€è¦åŒ…å«é«˜ç¨‹å›¾åºåˆ—æ•°æ®ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        # å®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å…·ä½“è¾“å…¥æ ¼å¼è°ƒæ•´
        obs_len = self.normalizer.in_features
        proprio_obs = x[:, 0:obs_len]
        height_data = x[:, obs_len:]  # åŒ…å«é«˜ç¨‹å›¾åºåˆ—æ•°æ®
        
        # å½’ä¸€åŒ–æœ¬ä½“è§‚æµ‹
        normalized_obs = self.normalizer(proprio_obs)
        
        # é‡å¡‘é«˜ç¨‹å›¾æ•°æ®ï¼ˆå‡è®¾è¾“å…¥æ˜¯å±•å¹³çš„ï¼‰
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¾“å…¥æ ¼å¼è°ƒæ•´
        batch_size = x.shape[0]
        height, width = self.vision_spatial_size
        sequence_length = height_data.shape[1] // (height * width)
        height_map_sequence = height_data.view(batch_size, sequence_length, height, width)
        
        # æå–ç‰¹å¾
        proprio_features = self.proprio_encoder(normalized_obs)
        vision_features = self.elevation_net(height_map_sequence)
        
        # èåˆç‰¹å¾
        fused_features = torch.cat([proprio_features, vision_features], dim=-1)
        x = self.fusion_encoder(fused_features)
        
        # VAEç¼–ç ï¼šä½¿ç”¨å‡å€¼ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        latent_mean = self.encoder_latent_mean(x)
        vel_mean = self.encoder_vel_mean(x)
        
        # æ‹¼æ¥éšå‘é‡
        code = torch.cat((vel_mean, latent_mean), dim=-1)
        
        # ä¸å½“å‰æœ¬ä½“è§‚æµ‹æ‹¼æ¥
        now_obs = normalized_obs[:, 0:self.obs_one_frame_len]
        observation = torch.cat((code.detach(), now_obs), dim=-1)
        
        # è¾“å‡ºåŠ¨ä½œ
        actions_mean = self.actor(observation)
        return actions_mean, vel_mean

    def export(self, path, filename):
        self.to("cpu")
        self.eval()
        opset_version = 18
        # åˆ›å»ºè¾“å…¥ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        # å®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®é«˜ç¨‹å›¾çš„å®é™…å°ºå¯¸è®¡ç®—
        height, width = self.vision_spatial_size
        height_map_dim = height * width * 5  # å‡è®¾5å¸§
        total_dim = self.normalizer.in_features + height_map_dim
        obs = torch.zeros(1, total_dim)
        torch.onnx.export(
            self,
            obs,
            os.path.join(path, filename),
            export_params=True,
            opset_version=opset_version,
            verbose=self.verbose,
            input_names=["obs"],
            output_names=["actions", "est_vel"],
            dynamic_axes={},
        )
